{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Working with large data sets in Keras</b>\n",
    "\n",
    "Imran A. Zualkernan\n",
    "\n",
    "March 2, 2021\n",
    "\n",
    "The purpose of this notebook is to demonstrate how generators can be used to create Keras models from large amounts of data.  \n",
    "\n",
    "Before using this notebook please download data on images of bird species from <i> https://www.kaggle.com/gpiosenka/100-bird-species/code </i> into the same directory containing this notebook. \n",
    "\n",
    "<b> v1.1 </b>\n",
    "<hr>\n",
    "<i> copyright Imran Zualkernan </i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful links \n",
    "# https://www.hostinger.com/tutorials/ssh/basic-ssh-commands\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "print(tf.test.is_built_with_cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image, display\n",
    "import random\n",
    "import math\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Model, Sequential\n",
    "from keras.utils import plot_model \n",
    "from keras.layers import Input, Dense, Dropout, Flatten, Activation,Concatenate\n",
    "from keras.layers import Conv2D, MaxPooling2D, AveragePooling2D\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend, models\n",
    "#import tensorflow_addons as tfa\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n",
    "# need to add these for the GPU\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.compat.v1.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the image generator\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 35215 images belonging to 250 classes.\n",
      "Found 1250 images belonging to 250 classes.\n",
      "Found 1250 images belonging to 250 classes.\n"
     ]
    }
   ],
   "source": [
    "#Setting the parameters for training\n",
    "\n",
    "# batch size and image width to use\n",
    "batch_size=128\n",
    "width=100\n",
    "\n",
    "# all the data directories\n",
    "train_dir='train/';\n",
    "test_dir='test/'\n",
    "valid_dir='valid/';\n",
    "\n",
    "# the number of epochs\n",
    "num_epochs=10\n",
    "\n",
    "# creating an image generator that will feed the data from\n",
    "# each of the directories\n",
    "\n",
    "# we use scaling transformation in this generator\n",
    "generator=ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# we specify the size of the input and batch size\n",
    "# size of the input is necessary because the image\n",
    "# needs to be rescaled for the neural network\n",
    "\n",
    "train_data=generator.flow_from_directory(train_dir, target_size=(width,width),batch_size=batch_size)\n",
    "valid_data=generator.flow_from_directory(valid_dir, target_size=(width,width),batch_size=batch_size)\n",
    "test_data=generator.flow_from_directory(test_dir, target_size=(width,width),batch_size=batch_size)\n",
    "\n",
    "# the number of steps per epoch is samples/batch size\n",
    "# we need to use these numbers later\n",
    "\n",
    "train_steps_per_epoch=math.ceil(train_data.samples/batch_size)\n",
    "valid_steps_per_epoch=math.ceil(valid_data.samples/batch_size)\n",
    "test_steps_per_epoch=math.ceil(test_data.samples/batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the actual model should go here \n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), input_shape=(width, width, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(256, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(250, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer='adam', \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tensorflow.python.keras.engine.sequential.Sequential object at 0x7ff7eac3b7b8>\n"
     ]
    }
   ],
   "source": [
    "# see if the model is good. \n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "Epoch 1/20\n",
      "276/276 [==============================] - 28s 102ms/step - loss: 3.0539 - accuracy: 0.3134 - val_loss: 2.1919 - val_accuracy: 0.4896\n",
      "Epoch 2/20\n",
      "276/276 [==============================] - 28s 102ms/step - loss: 2.4730 - accuracy: 0.4209 - val_loss: 1.8123 - val_accuracy: 0.5784\n",
      "Epoch 3/20\n",
      "276/276 [==============================] - 28s 102ms/step - loss: 2.1054 - accuracy: 0.4945 - val_loss: 1.5240 - val_accuracy: 0.6360\n",
      "Epoch 4/20\n",
      "276/276 [==============================] - 28s 102ms/step - loss: 1.8287 - accuracy: 0.5496 - val_loss: 1.3678 - val_accuracy: 0.6656\n",
      "Epoch 5/20\n",
      "276/276 [==============================] - 28s 102ms/step - loss: 1.6104 - accuracy: 0.5940 - val_loss: 1.3415 - val_accuracy: 0.6768\n",
      "Epoch 6/20\n",
      "276/276 [==============================] - 28s 102ms/step - loss: 1.4287 - accuracy: 0.6300 - val_loss: 1.2973 - val_accuracy: 0.6728\n",
      "Epoch 7/20\n",
      "276/276 [==============================] - 28s 103ms/step - loss: 1.2971 - accuracy: 0.6607 - val_loss: 1.1867 - val_accuracy: 0.6920\n",
      "Epoch 8/20\n",
      "276/276 [==============================] - 28s 102ms/step - loss: 1.1719 - accuracy: 0.6891 - val_loss: 1.1335 - val_accuracy: 0.7136\n",
      "Epoch 9/20\n",
      "276/276 [==============================] - 28s 101ms/step - loss: 1.0557 - accuracy: 0.7130 - val_loss: 1.2822 - val_accuracy: 0.6976\n",
      "Epoch 10/20\n",
      "276/276 [==============================] - 28s 102ms/step - loss: 0.9657 - accuracy: 0.7352 - val_loss: 1.1937 - val_accuracy: 0.6920\n",
      "Epoch 11/20\n",
      "276/276 [==============================] - 28s 102ms/step - loss: 0.8873 - accuracy: 0.7504 - val_loss: 1.1710 - val_accuracy: 0.7240\n",
      "Epoch 12/20\n",
      "276/276 [==============================] - 28s 102ms/step - loss: 0.8278 - accuracy: 0.7648 - val_loss: 1.1732 - val_accuracy: 0.7136\n",
      "Epoch 13/20\n",
      "276/276 [==============================] - 28s 102ms/step - loss: 0.7579 - accuracy: 0.7807 - val_loss: 1.1679 - val_accuracy: 0.7248\n",
      "Epoch 14/20\n",
      "276/276 [==============================] - 28s 103ms/step - loss: 0.7114 - accuracy: 0.7928 - val_loss: 1.2501 - val_accuracy: 0.7104\n",
      "Epoch 15/20\n",
      "276/276 [==============================] - 28s 103ms/step - loss: 0.6583 - accuracy: 0.8058 - val_loss: 1.2085 - val_accuracy: 0.7200\n",
      "Epoch 16/20\n",
      "276/276 [==============================] - 28s 102ms/step - loss: 0.6262 - accuracy: 0.8158 - val_loss: 1.1525 - val_accuracy: 0.7296\n",
      "Epoch 17/20\n",
      "276/276 [==============================] - 28s 102ms/step - loss: 0.5846 - accuracy: 0.8275 - val_loss: 1.1843 - val_accuracy: 0.7144\n",
      "Epoch 18/20\n",
      "276/276 [==============================] - 28s 102ms/step - loss: 0.5486 - accuracy: 0.8363 - val_loss: 1.2760 - val_accuracy: 0.7080\n",
      "Epoch 19/20\n",
      "276/276 [==============================] - 28s 103ms/step - loss: 0.5151 - accuracy: 0.8439 - val_loss: 1.2261 - val_accuracy: 0.7320\n",
      "Epoch 20/20\n",
      "276/276 [==============================] - 28s 101ms/step - loss: 0.5131 - accuracy: 0.8460 - val_loss: 1.1976 - val_accuracy: 0.7256\n"
     ]
    }
   ],
   "source": [
    "print(valid_steps_per_epoch)\n",
    "num_epochs = 20\n",
    "history=model.fit(train_data,\n",
    "                  steps_per_epoch =train_steps_per_epoch, \n",
    "                  validation_data=valid_data,\n",
    "                  epochs=num_epochs,\n",
    "                  validation_steps=valid_steps_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "from keras import metrics\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer='adam', \n",
    "              metrics=['accuracy',\n",
    "                        metrics.AUC(name='my_auc'),\n",
    "                        F1_Score])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://keras.io/api/callbacks/\n",
    "# We can use a variety of pre-defined callbacks.\n",
    "# Experiment with ReduceLROnPlateuau()\n",
    "\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, CSVLogger\n",
    "\n",
    "# We can also do a modelcheck point \n",
    "# https://machinelearningmastery.com/check-point-deep-learning-models-keras/\n",
    "  \n",
    "# checkpoint to save the model with best validation accuracy\n",
    "checkpoint = ModelCheckpoint(filepath='model.{epoch:02d}-{val_loss:.2f}.h5', \n",
    "                             monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "# We can also stop the model early\n",
    "#https://machinelearningmastery.com/how-to-stop-training-deep-neural-networks-at-the-right-time-using-early-stopping/\n",
    "# val_loss\n",
    "early_stopping_callback = EarlyStopping(monitor='accuracy', mode='min', verbose=1, patience=200)\n",
    "\n",
    "\n",
    "# initialize TimeStopping callback\n",
    "# https://www.tensorflow.org/addons/tutorials/time_stopping\n",
    "# note that it will still run a minimum of 1 epoch\n",
    "time_stopping_callback = tfa.callbacks.TimeStopping(seconds=600, verbose=1)\n",
    "\n",
    "# We can also use CVSLogger to log information in a CSV\n",
    "csvlogger = CSVLogger(\"logfile.csv\",separator=',',append=False)\n",
    "\n",
    "\n",
    "# ** IMPORTANT ** - please make sure that csvlogger is the last call back\n",
    "# in the list.\n",
    "\n",
    "my_callbacks = [time_stopping_callback,early_stopping_callback,checkpoint,csvlogger]\n",
    "\n",
    "                                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the model with call-backs\n",
    "\n",
    "num_epochs = 1\n",
    "\n",
    "history=model.fit(train_data,\n",
    "                  steps_per_epoch =train_steps_per_epoch, \n",
    "                  validation_data=valid_data,\n",
    "                  epochs=num_epochs,\n",
    "                  validation_steps=valid_steps_per_epoch,\n",
    "                  callbacks=my_callbacks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "from keras import metrics\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer='adam', \n",
    "              metrics=['accuracy',\n",
    "                        metrics.AUC(name='auc'),\n",
    "                        metrics.Precision(name='precision'),\n",
    "                        metrics.Recall(name='recall')])\n",
    "\n",
    "# Fitting the model with more metrics\n",
    "\n",
    "num_epochs = 1\n",
    "\n",
    "history=model.fit(train_data,\n",
    "                  steps_per_epoch =train_steps_per_epoch, \n",
    "                  validation_data=valid_data,\n",
    "                  epochs=num_epochs,\n",
    "                  validation_steps=valid_steps_per_epoch,\n",
    "                  callbacks=my_callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining custom metrics to record while running\n",
    "from keras import backend as K\n",
    "\n",
    "def F1_Score(y_true, y_pred): #taken from old keras source code\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n",
    "    return f1_val\n",
    "\n",
    "def my_metric_fn(y_true, y_pred):\n",
    "    squared_difference = tf.square(y_true - y_pred)\n",
    "    return tf.reduce_mean(squared_difference, axis=-1)  # Note the `axis=-1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "from keras import metrics\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer='adam', \n",
    "              metrics=['accuracy',\n",
    "                        metrics.AUC(name='auc'),\n",
    "                        metrics.Precision(name='precision'),\n",
    "                        metrics.Recall(name='recall'),\n",
    "                        F1_Score])\n",
    "\n",
    "# Fitting the model with more metrics\n",
    "\n",
    "num_epochs = 1\n",
    "\n",
    "history=model.fit(train_data,\n",
    "                  steps_per_epoch =train_steps_per_epoch, \n",
    "                  validation_data=valid_data,\n",
    "                  epochs=num_epochs,\n",
    "                  validation_steps=valid_steps_per_epoch,\n",
    "                  callbacks=my_callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining custom call backs\n",
    "\n",
    "# https://www.tensorflow.org/guide/keras/custom_callback\n",
    "# https://keras.io/guides/writing_your_own_callbacks/\n",
    "\n",
    "from keras.callbacks import Callback\n",
    "import time\n",
    "\n",
    "class TimingCallback(keras.callbacks.Callback):\n",
    "    def __init__(self):\n",
    "        super(TimingCallback, self).__init__()\n",
    "    def on_batch_begin(self, epoch, logs=None):\n",
    "        self.starttime=time.time()\n",
    "    def on_batch_end(self, epoch, logs=None):\n",
    "        logs['epoch_time'] = (time.time()-self.starttime)\n",
    "        print('\\nepoch_time(sec)=',logs['epoch_time'],'\\n')\n",
    "        \n",
    "# create an instance of the timingcallback\n",
    "timing_call = TimingCallback() \n",
    "\n",
    "# We can also use other metrics\n",
    "# https://keras.io/api/metrics/\n",
    "class PrintBatchCallback(keras.callbacks.Callback):  \n",
    "    def on_train_batch_end(self, batch, logs=None):\n",
    "        print(\"For batch {}, loss is {:7.2f}.\".format(batch, logs[\"loss\"]))\n",
    "        print(\"For batch {}, accuracy is {:7.2f}.\".format(batch, logs[\"accuracy\"]))\n",
    "        print(\"For batch {}, AUC is {:7.2f}.\".format(batch, logs[\"auc\"]))\n",
    "\n",
    "print_batch_call = PrintBatchCallback()\n",
    "\n",
    "# add to the callback list\n",
    "my_callbacks = [time_stopping_callback,early_stopping_callback,checkpoint,print_batch_call, timing_call, CSVLogger('new.csv', separator=',')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "from keras import metrics\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer='adam', \n",
    "              metrics=['accuracy',\n",
    "                        metrics.AUC(name='auc'),\n",
    "                        metrics.Precision(name='precision'),\n",
    "                        metrics.Recall(name='recall'),\n",
    "                        F1_Score])\n",
    "\n",
    "# Fitting the model with more metrics\n",
    "\n",
    "num_epochs = 1\n",
    "\n",
    "history=model.fit(train_data,\n",
    "                  steps_per_epoch =train_steps_per_epoch, \n",
    "                  validation_data=valid_data,\n",
    "                  epochs=num_epochs,\n",
    "                  validation_steps=valid_steps_per_epoch,\n",
    "                  callbacks=my_callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://neptune.ai/blog/keras-metrics\n",
    "\n",
    "# How to save batch level data in a file \n",
    "\n",
    "import os\n",
    "from keras.callbacks import Callback\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class SaveBatchLevelDataCallback(keras.callbacks.Callback):\n",
    "    def __init__(self, validation_data, save_dir):\n",
    "        super().__init__()\n",
    "        self.validation_data = validation_data\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        self.save_dir = save_dir\n",
    "        self.f = None\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        # create a file\n",
    "        self.f= open(os.path.join(self.save_dir, f'epoch_{epoch}.csv'),'w+')\n",
    "        line = \"batch,loss,accuracy,auc\\n\"\n",
    "        self.f.write(line)\n",
    "    \n",
    "    def on_epoch_end(self, batch, logs=None):\n",
    "        self.f.close()\n",
    "        \n",
    "    def on_train_batch_end(self, batch, logs=None):\n",
    "        line = \"{},{:7.2f},{:7.2f},{:7.2f}\\n\".format(batch, logs[\"loss\"], logs[\"accuracy\"],logs[\"auc\"])\n",
    "        self.f.write(line)\n",
    "        \n",
    "    \n",
    "batch_write_cbk = SaveBatchLevelDataCallback(validation_data=valid_data,save_dir='batch_data')\n",
    "\n",
    "# add to the callback list\n",
    "my_callbacks = [time_stopping_callback,early_stopping_callback,checkpoint,batch_write_cbk, CSVLogger('new.csv', separator=',')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Compile the model\n",
    "from keras import metrics\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer='adam', \n",
    "              metrics=['accuracy',\n",
    "                        metrics.AUC(name='auc'),\n",
    "                        metrics.Precision(name='precision'),\n",
    "                        metrics.Recall(name='recall'),\n",
    "                        F1_Score])\n",
    "\n",
    "# Fitting the model with more metrics\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "history=model.fit(train_data,\n",
    "                  steps_per_epoch =train_steps_per_epoch, \n",
    "                  validation_data=valid_data,\n",
    "                  epochs=num_epochs,\n",
    "                  validation_steps=valid_steps_per_epoch,\n",
    "                  callbacks=my_callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print history \n",
    "print(history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot accuracy vs epoch\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validate'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot loss values vs epoch\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validate'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot loss values vs epoch\n",
    "plt.plot(history.history['F1_Score'])\n",
    "plt.plot(history.history['val_F1_Score'])\n",
    "plt.title('Model F1-Score')\n",
    "plt.ylabel('F1_Score')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validate'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot accuracy vs. prevision\n",
    "plt.plot(history.history['precision'],label='precision')\n",
    "plt.plot(history.history['val_precision'],label='val_precision')\n",
    "plt.plot(history.history['recall'],label='recall')\n",
    "plt.plot(history.history['val_recall'],label='val_precision')\n",
    "plt.title('Model Precision and Recall')\n",
    "plt.ylabel('Precision and Recall')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot accuracy vs. prevision\n",
    "plt.plot(history.history['precision'],history.history['recall'],'o', color='black',label='precision vs. recall')\n",
    "plt.plot(history.history['recall'],history.history['val_recall'],'o', color='red',label='val_precision vs. val_recall')\n",
    "plt.title('Model Precision and Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.xlabel('Recall')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Evaluate against test data.\n",
    "scores = model.evaluate(test_data, verbose=1)\n",
    "\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])\n",
    "print('Test AUC:', scores[1])\n",
    "print('Test precision:', scores[1])\n",
    "print('Test recall:', scores[1])\n",
    "print('Test F1-Score:', scores[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For evaluation first, we will create the actual and predicted labels\n",
    "# We can then use these to generate all the reports we need.\n",
    "\n",
    "# make predictions on the testing images, finding the index of the\n",
    "# label with the corresponding largest predicted probability\n",
    "\n",
    "predicted = model.predict(x=test_data, steps=test_steps_per_epoch)\n",
    "\n",
    "# create predited IDs\n",
    "predicted = np.argmax(predicted, axis=1)\n",
    "\n",
    "# create test labels from the generator\n",
    "actual = []\n",
    "for i in range(0,int(test_steps_per_epoch)):\n",
    "    actual.extend(np.array(test_data[i][1]))\n",
    "\n",
    "# create actual IDs\n",
    "actual = np.asarray(actual).argmax(axis=1)\n",
    "\n",
    "# make sure predicted and actual are the same size and shape\n",
    "print(predicted.shape)\n",
    "print(actual.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(\"[INFO] evaluating network...\")\n",
    "print(classification_report(actual, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can determine the confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm=confusion_matrix(actual,predicted)\n",
    "\n",
    "def print_cm(cm, frm, to,abs_or_relative=0):\n",
    "    import seaborn as sns\n",
    "    import matplotlib.pylab as plt\n",
    "\n",
    "    cm = cm[frm:to+1,frm:to+1]\n",
    "    # create labels\n",
    "    x_axis_labels = np.arange(frm,to+1)\n",
    "    y_axis_labels = np.arange(frm,to+1)\n",
    "    \n",
    "    plt.xticks(rotation=45)\n",
    "    plt.yticks(rotation=-45)\n",
    "    \n",
    "    if(abs_or_relative==0):\n",
    "        sns.heatmap(cm, annot=True,xticklabels=x_axis_labels, yticklabels=y_axis_labels)\n",
    "    else:\n",
    "        sns.heatmap(cm/np.sum(cm), annot=True, \n",
    "           fmt='.2%', cmap='Blues',\n",
    "           xticklabels=x_axis_labels, yticklabels=y_axis_labels)\n",
    "\n",
    "print_cm(cm,1 ,20,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we already have actual and predicted \n",
    "\n",
    "# also see https://www.dlology.com/blog/simple-guide-on-how-to-generate-roc-plot-for-keras-classifier/\n",
    "# for micro-average ROC curves as well\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "\n",
    "#extract the actual labels from the test data\n",
    "Y_test = []\n",
    "for i in range(0,int(test_steps_per_epoch)):\n",
    "    Y_test.extend(np.array(test_data[i][1]))\n",
    "Y_test = np.array(Y_test)\n",
    "n_classes = Y_test.shape[1]  # one hot encoded\n",
    "\n",
    "# create actual output from the model using test_data\n",
    "y_score=model.predict(x=test_data, steps=test_steps_per_epoch)\n",
    "\n",
    "print(Y_test.shape)\n",
    "print(y_score.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(n_classes)\n",
    "# compare each class's probabilities one by one\n",
    "# each acts like a single column\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(Y_test[:,i], y_score[:,i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# Print the AUC scores\n",
    "from IPython.display import display\n",
    "import pandas as pd\n",
    "auc_array = np.array(list(roc_auc.items()))\n",
    "df = pd.DataFrame(auc_array[:,1])\n",
    "df.columns = ['AUC']\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the ROC for the ith class cls\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "def plot_roc(cls,roc_dir):  \n",
    "    plt.plot(fpr[cls], tpr[cls], lw=2,label='ROC curve of class {0} (area = {1:0.3f})'\n",
    "    ''.format(cls, roc_auc[cls]))\n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(roc_dir, f'ROC_{cls}.png'))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# make sure directory exists\n",
    "def make_directory(roc_dir):\n",
    "    try:\n",
    "        os.mkdir(roc_dir)\n",
    "    except OSError:\n",
    "        print (\"Creation of the directory %s failed\" % roc_dir)\n",
    "    else:\n",
    "        print (\"Successfully created the directory %s \" % roc_dir)\n",
    "        \n",
    "# print the roc curve for 0\n",
    "\n",
    "make_directory('rocs')\n",
    "\n",
    "for i in range(n_classes):\n",
    "    plot_roc(i,'rocs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using tensorflow extension\n",
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define tensorboard callback\n",
    "\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "# Using remote tensorboard\n",
    "#https://blog.yyliu.net/remote-tensorboard/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "from keras import metrics\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer='adam', \n",
    "              metrics=['accuracy',\n",
    "                        metrics.AUC(name='auc'),\n",
    "                        metrics.Precision(name='precision'),\n",
    "                        metrics.Recall(name='recall')])\n",
    "\n",
    "# Fitting the model with more metrics\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "history=model.fit(train_data,\n",
    "                  steps_per_epoch =train_steps_per_epoch, \n",
    "                  validation_data=valid_data,\n",
    "                  epochs=num_epochs,\n",
    "                  validation_steps=valid_steps_per_epoch,\n",
    "                  callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
