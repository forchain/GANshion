{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COE59410- Generative Deep Learning\n",
    "\n",
    "## Homework 1\n",
    "\n",
    "> Imran A. Zualkernan\n",
    "\n",
    "The purpose of this homework is to learn how to use the GPU machines to run large models. In addition, build familiarity with the Keras framework and the associated tools.\n",
    "\n",
    "## Deliverables\n",
    "\n",
    "**You need to upload the following on ilearn (2 items)**\n",
    "\n",
    "1. The Jupyter notebook in its original format.\n",
    "\n",
    "2. A PDF of the Jupyter notebook for grading.\n",
    "\n",
    "*Please do not upload a zipped file.* Upload each file separately. Each question is worth 25 points.\n",
    "\n",
    "* Q1. Load and run the large_scale_processing v1.1 Jupyter notebook on the GPU machine and show how you can use tensorboard to monitor the runs remotely on your local machine.\n",
    "\n",
    "* Q2. Modify the model in large_scale_processing v1.1 so that rather than a CNN, the model is a fully connected feedforward neural network. Fine tune the model to show your best results. Report and discuss all the results that are necessary to determine the goodness of your best model.\n",
    "\n",
    "> Hint: Use the Reshape Layer in Keras.\n",
    "\n",
    "* Q3. Use the following two call-backs on your best fully connected model and determine if you are able to improve the results. Clearly explain why or why not.\n",
    "    1. LearningRateScheduler\n",
    "    2. ReduceLROnPlateau\n",
    "\n",
    "* Q4. Use the Keras Hypertune and Random optimizers (https://keras-team.github.io/keras-tuner/) to determine if you can improve the model by varying the number of layers, neurons in each layer and the learning rate.\n",
    "\n",
    "    1. Plot the precision vs. recall of the best 20 models in one figure.\n",
    "    2. Show a complete evaluation of the top two models.\n",
    "\n",
    "## Group 2\n",
    "* Eman ,\n",
    "* Huangjin Zhou, b00080932\n",
    "* Mueez ,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful links \n",
    "# https://www.hostinger.com/tutorials/ssh/basic-ssh-commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA runtime implicit initialization on GPU:0 failed. Status: out of memory",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-4-5a178bb5b7a5>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mclient\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mdevice_lib\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdevice_lib\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlist_local_devices\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      3\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/tf_gpu/lib/python3.7/site-packages/tensorflow/python/client/device_lib.py\u001B[0m in \u001B[0;36mlist_local_devices\u001B[0;34m(session_config)\u001B[0m\n\u001B[1;32m     41\u001B[0m     \u001B[0mserialized_config\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0msession_config\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mSerializeToString\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     42\u001B[0m   return [\n\u001B[0;32m---> 43\u001B[0;31m       \u001B[0m_convert\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0ms\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0ms\u001B[0m \u001B[0;32min\u001B[0m \u001B[0m_pywrap_device_lib\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlist_devices\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mserialized_config\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     44\u001B[0m   ]\n",
      "\u001B[0;31mRuntimeError\u001B[0m: CUDA runtime implicit initialization on GPU:0 failed. Status: out of memory"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# print(tf.config.list_physical_devices('GPU'))\n",
    "tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.list_physical_devices(device_type=None)\n",
    "tf.test.is_gpu_available()\n",
    "print(tf.test.is_built_with_cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image, display\n",
    "import random\n",
    "import math\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Model, Sequential\n",
    "from keras.utils import plot_model \n",
    "from keras.layers import Reshape, Input, Dense, Dropout, Flatten, Activation,Concatenate\n",
    "from keras.layers import Conv2D, MaxPooling2D, AveragePooling2D\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend, models\n",
    "#import tensorflow_addons as tfa\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n",
    "# need to add these for the GPU\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.compat.v1.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the image generator\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting the parameters for training\n",
    "\n",
    "# batch size and image width to use\n",
    "batch_size=128\n",
    "width=100\n",
    "\n",
    "# all the data directories\n",
    "train_dir='train/'\n",
    "test_dir='test/'\n",
    "valid_dir='valid/'\n",
    "\n",
    "# the number of epochs\n",
    "num_epochs=10\n",
    "\n",
    "# creating an image generator that will feed the data from\n",
    "# each of the directories\n",
    "\n",
    "# we use scaling transformation in this generator\n",
    "generator=ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# we specify the size of the input and batch size\n",
    "# size of the input is necessary because the image\n",
    "# needs to be rescaled for the neural network\n",
    "\n",
    "train_data=generator.flow_from_directory(train_dir, target_size=(width,width),batch_size=batch_size)\n",
    "valid_data=generator.flow_from_directory(valid_dir, target_size=(width,width),batch_size=batch_size)\n",
    "test_data=generator.flow_from_directory(test_dir, target_size=(width,width),batch_size=batch_size)\n",
    "\n",
    "# the number of steps per epoch is samples/batch size\n",
    "# we need to use these numbers later\n",
    "\n",
    "train_steps_per_epoch=math.ceil(train_data.samples/batch_size)\n",
    "valid_steps_per_epoch=math.ceil(valid_data.samples/batch_size)\n",
    "test_steps_per_epoch=math.ceil(test_data.samples/batch_size)\n",
    "print(train_steps_per_epoch)\n",
    "print(valid_steps_per_epoch)\n",
    "print(test_steps_per_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "> Q1. Load and run the large_scale_processing v1.1 Jupyter notebook on the GPU machine and\n",
    "show how you can use tensorboard to monitor the runs remotely on your local machine.\n",
    "\n",
    "\n",
    "```shell\n",
    "$ jupyter notebook --port 9999 --NotebookApp.allow_remote_access=True\n",
    "$ tensorboard --logdir logs/fit --port=8888\n",
    "```\n",
    "> Scalars\n",
    "![Scalars](images/scalar.jpg)\n",
    "![Graph](images/graph.jpg)\n",
    "![Time Series](images/timeseries.jpg)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1 = True\n",
    "if Q1:\n",
    "    # the actual model should go here\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), input_shape=(width, width, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Conv2D(256, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1024, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(250, activation='softmax'))\n",
    "    model.summary()\n",
    "\n",
    "    from tensorflow.keras.callbacks import TensorBoard\n",
    "    tensorboard = TensorBoard(log_dir='logs/fit')\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'], callbacks=[tensorboard])\n",
    "exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "hidden_units = 256*4\n",
    "dropout = 0.1\n",
    "num_labels = train_data.num_classes\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Reshape((-1,), input_shape=(width, width, 3)))\n",
    "model.add(Dense(hidden_units, activation='relu'))\n",
    "# model.add(Dropout(dropout))\n",
    "model.add(Dense(hidden_units, activation='relu'))\n",
    "# model.add(Dropout(dropout))\n",
    "model.add(Dense(num_labels, activation='softmax'))\n",
    "model.summary()\n",
    "\n",
    "\n",
    "from kerastuner.tuners import RandomSearch\n",
    "\n",
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "    model.add(Reshape((-1,), input_shape=(width, width, 3)))\n",
    "    model.add(Dense(units=hp.Int('units',\n",
    "                                        min_value=128,\n",
    "                                        max_value=1024,\n",
    "                                        step=128),\n",
    "                           activation='relu'))\n",
    "    model.add(Dense(units=hp.Int('units',\n",
    "                                        min_value=128,\n",
    "                                        max_value=1024,\n",
    "                                        step=128),\n",
    "                           activation='relu'))\n",
    "    model.add(Dense(units=hp.Int('units',\n",
    "                                        min_value=128,\n",
    "                                        max_value=1024,\n",
    "                                        step=128),\n",
    "                           activation='relu'))\n",
    "    model.add(Dense(num_labels, activation='softmax'))\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(\n",
    "            hp.Choice('learning_rate',\n",
    "                      values=[1e-2, 1e-3, 1e-4])),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy'])\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer='adam', \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see if the model is good. \n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import LearningRateScheduler, ReduceLROnPlateau\n",
    "\n",
    "def lr_schedule(epoch):\n",
    "    \"\"\"Learning rate scheduler - called every epoch\"\"\"\n",
    "    lr = 1e-3\n",
    "    fold = int(epoch / 10) + 1\n",
    "    lr /=  fold\n",
    "\n",
    "    return lr\n",
    "\n",
    "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "\n",
    "lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1),\n",
    "                               cooldown=0,\n",
    "                               patience=5,\n",
    "                               min_lr=0.5e-6)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "tensorboard = TensorBoard(log_dir='logs/fit')\n",
    "\n",
    "print(valid_steps_per_epoch)\n",
    "num_epochs = 20\n",
    "\n",
    "callbacks = [lr_reducer, lr_scheduler, tensorboard]\n",
    "history=model.fit(train_data,\n",
    "                  steps_per_epoch =train_steps_per_epoch,\n",
    "                  validation_data=valid_data,\n",
    "                  epochs=num_epochs,\n",
    "                  validation_steps=valid_steps_per_epoch, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "from keras import metrics\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer='adam', \n",
    "              metrics=['accuracy',\n",
    "                        metrics.AUC(name='my_auc'),\n",
    "                        F1_Score])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://keras.io/api/callbacks/\n",
    "# We can use a variety of pre-defined callbacks.\n",
    "# Experiment with ReduceLROnPlateuau()\n",
    "\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, CSVLogger, LearningRateScheduler\n",
    "\n",
    "# We can also do a modelcheck point \n",
    "# https://machinelearningmastery.com/check-point-deep-learning-models-keras/\n",
    "  \n",
    "# checkpoint to save the model with best validation accuracy\n",
    "checkpoint = ModelCheckpoint(filepath='model.{epoch:02d}-{val_loss:.2f}.h5', \n",
    "                             monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "# We can also stop the model early\n",
    "#https://machinelearningmastery.com/how-to-stop-training-deep-neural-networks-at-the-right-time-using-early-stopping/\n",
    "# val_loss\n",
    "early_stopping_callback = EarlyStopping(monitor='accuracy', mode='min', verbose=1, patience=200)\n",
    "\n",
    "\n",
    "# initialize TimeStopping callback\n",
    "# https://www.tensorflow.org/addons/tutorials/time_stopping\n",
    "# note that it will still run a minimum of 1 epoch\n",
    "time_stopping_callback = tfa.callbacks.TimeStopping(seconds=600, verbose=1)\n",
    "\n",
    "# We can also use CVSLogger to log information in a CSV\n",
    "csvlogger = CSVLogger(\"logfile.csv\",separator=',',append=False)\n",
    "\n",
    "\n",
    "# ** IMPORTANT ** - please make sure that csvlogger is the last call back\n",
    "# in the list.\n",
    "\n",
    "my_callbacks = [time_stopping_callback,early_stopping_callback,checkpoint,csvlogger]\n",
    "\n",
    "                                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the model with call-backs\n",
    "\n",
    "num_epochs = 1\n",
    "\n",
    "history=model.fit(train_data,\n",
    "                  steps_per_epoch =train_steps_per_epoch, \n",
    "                  validation_data=valid_data,\n",
    "                  epochs=num_epochs,\n",
    "                  validation_steps=valid_steps_per_epoch,\n",
    "                  callbacks=my_callbacks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "from keras import metrics\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer='adam', \n",
    "              metrics=['accuracy',\n",
    "                        metrics.AUC(name='auc'),\n",
    "                        metrics.Precision(name='precision'),\n",
    "                        metrics.Recall(name='recall')])\n",
    "\n",
    "# Fitting the model with more metrics\n",
    "\n",
    "num_epochs = 1\n",
    "\n",
    "history=model.fit(train_data,\n",
    "                  steps_per_epoch =train_steps_per_epoch, \n",
    "                  validation_data=valid_data,\n",
    "                  epochs=num_epochs,\n",
    "                  validation_steps=valid_steps_per_epoch,\n",
    "                  callbacks=my_callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining custom metrics to record while running\n",
    "from keras import backend as K\n",
    "\n",
    "def F1_Score(y_true, y_pred): #taken from old keras source code\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n",
    "    return f1_val\n",
    "\n",
    "def my_metric_fn(y_true, y_pred):\n",
    "    squared_difference = tf.square(y_true - y_pred)\n",
    "    return tf.reduce_mean(squared_difference, axis=-1)  # Note the `axis=-1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "from keras import metrics\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer='adam', \n",
    "              metrics=['accuracy',\n",
    "                        metrics.AUC(name='auc'),\n",
    "                        metrics.Precision(name='precision'),\n",
    "                        metrics.Recall(name='recall'),\n",
    "                        F1_Score])\n",
    "\n",
    "# Fitting the model with more metrics\n",
    "\n",
    "num_epochs = 1\n",
    "\n",
    "history=model.fit(train_data,\n",
    "                  steps_per_epoch =train_steps_per_epoch, \n",
    "                  validation_data=valid_data,\n",
    "                  epochs=num_epochs,\n",
    "                  validation_steps=valid_steps_per_epoch,\n",
    "                  callbacks=my_callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining custom call backs\n",
    "\n",
    "# https://www.tensorflow.org/guide/keras/custom_callback\n",
    "# https://keras.io/guides/writing_your_own_callbacks/\n",
    "\n",
    "from keras.callbacks import Callback\n",
    "import time\n",
    "\n",
    "class TimingCallback(keras.callbacks.Callback):\n",
    "    def __init__(self):\n",
    "        super(TimingCallback, self).__init__()\n",
    "    def on_batch_begin(self, epoch, logs=None):\n",
    "        self.starttime=time.time()\n",
    "    def on_batch_end(self, epoch, logs=None):\n",
    "        logs['epoch_time'] = (time.time()-self.starttime)\n",
    "        print('\\nepoch_time(sec)=',logs['epoch_time'],'\\n')\n",
    "        \n",
    "# create an instance of the timingcallback\n",
    "timing_call = TimingCallback() \n",
    "\n",
    "# We can also use other metrics\n",
    "# https://keras.io/api/metrics/\n",
    "class PrintBatchCallback(keras.callbacks.Callback):  \n",
    "    def on_train_batch_end(self, batch, logs=None):\n",
    "        print(\"For batch {}, loss is {:7.2f}.\".format(batch, logs[\"loss\"]))\n",
    "        print(\"For batch {}, accuracy is {:7.2f}.\".format(batch, logs[\"accuracy\"]))\n",
    "        print(\"For batch {}, AUC is {:7.2f}.\".format(batch, logs[\"auc\"]))\n",
    "\n",
    "print_batch_call = PrintBatchCallback()\n",
    "\n",
    "# add to the callback list\n",
    "my_callbacks = [time_stopping_callback,early_stopping_callback,checkpoint,print_batch_call, timing_call, CSVLogger('new.csv', separator=',')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "from keras import metrics\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer='adam', \n",
    "              metrics=['accuracy',\n",
    "                        metrics.AUC(name='auc'),\n",
    "                        metrics.Precision(name='precision'),\n",
    "                        metrics.Recall(name='recall'),\n",
    "                        F1_Score])\n",
    "\n",
    "# Fitting the model with more metrics\n",
    "\n",
    "num_epochs = 1\n",
    "\n",
    "history=model.fit(train_data,\n",
    "                  steps_per_epoch =train_steps_per_epoch, \n",
    "                  validation_data=valid_data,\n",
    "                  epochs=num_epochs,\n",
    "                  validation_steps=valid_steps_per_epoch,\n",
    "                  callbacks=my_callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://neptune.ai/blog/keras-metrics\n",
    "\n",
    "# How to save batch level data in a file \n",
    "\n",
    "import os\n",
    "from keras.callbacks import Callback\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class SaveBatchLevelDataCallback(keras.callbacks.Callback):\n",
    "    def __init__(self, validation_data, save_dir):\n",
    "        super().__init__()\n",
    "        self.validation_data = validation_data\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        self.save_dir = save_dir\n",
    "        self.f = None\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        # create a file\n",
    "        self.f= open(os.path.join(self.save_dir, f'epoch_{epoch}.csv'),'w+')\n",
    "        line = \"batch,loss,accuracy,auc\\n\"\n",
    "        self.f.write(line)\n",
    "    \n",
    "    def on_epoch_end(self, batch, logs=None):\n",
    "        self.f.close()\n",
    "        \n",
    "    def on_train_batch_end(self, batch, logs=None):\n",
    "        line = \"{},{:7.2f},{:7.2f},{:7.2f}\\n\".format(batch, logs[\"loss\"], logs[\"accuracy\"],logs[\"auc\"])\n",
    "        self.f.write(line)\n",
    "        \n",
    "    \n",
    "batch_write_cbk = SaveBatchLevelDataCallback(validation_data=valid_data,save_dir='batch_data')\n",
    "\n",
    "# add to the callback list\n",
    "my_callbacks = [time_stopping_callback,early_stopping_callback,checkpoint,batch_write_cbk, CSVLogger('new.csv', separator=',')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Compile the model\n",
    "from keras import metrics\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer='adam', \n",
    "              metrics=['accuracy',\n",
    "                        metrics.AUC(name='auc'),\n",
    "                        metrics.Precision(name='precision'),\n",
    "                        metrics.Recall(name='recall'),\n",
    "                        F1_Score])\n",
    "\n",
    "# Fitting the model with more metrics\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "history=model.fit(train_data,\n",
    "                  steps_per_epoch =train_steps_per_epoch, \n",
    "                  validation_data=valid_data,\n",
    "                  epochs=num_epochs,\n",
    "                  validation_steps=valid_steps_per_epoch,\n",
    "                  callbacks=my_callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print history \n",
    "print(history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot accuracy vs epoch\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validate'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot loss values vs epoch\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validate'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot loss values vs epoch\n",
    "plt.plot(history.history['F1_Score'])\n",
    "plt.plot(history.history['val_F1_Score'])\n",
    "plt.title('Model F1-Score')\n",
    "plt.ylabel('F1_Score')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validate'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot accuracy vs. prevision\n",
    "plt.plot(history.history['precision'],label='precision')\n",
    "plt.plot(history.history['val_precision'],label='val_precision')\n",
    "plt.plot(history.history['recall'],label='recall')\n",
    "plt.plot(history.history['val_recall'],label='val_precision')\n",
    "plt.title('Model Precision and Recall')\n",
    "plt.ylabel('Precision and Recall')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot accuracy vs. prevision\n",
    "plt.plot(history.history['precision'],history.history['recall'],'o', color='black',label='precision vs. recall')\n",
    "plt.plot(history.history['recall'],history.history['val_recall'],'o', color='red',label='val_precision vs. val_recall')\n",
    "plt.title('Model Precision and Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.xlabel('Recall')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Evaluate against test data.\n",
    "scores = model.evaluate(test_data, verbose=1)\n",
    "\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])\n",
    "print('Test AUC:', scores[1])\n",
    "print('Test precision:', scores[1])\n",
    "print('Test recall:', scores[1])\n",
    "print('Test F1-Score:', scores[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For evaluation first, we will create the actual and predicted labels\n",
    "# We can then use these to generate all the reports we need.\n",
    "\n",
    "# make predictions on the testing images, finding the index of the\n",
    "# label with the corresponding largest predicted probability\n",
    "\n",
    "predicted = model.predict(x=test_data, steps=test_steps_per_epoch)\n",
    "\n",
    "# create predited IDs\n",
    "predicted = np.argmax(predicted, axis=1)\n",
    "\n",
    "# create test labels from the generator\n",
    "actual = []\n",
    "for i in range(0,int(test_steps_per_epoch)):\n",
    "    actual.extend(np.array(test_data[i][1]))\n",
    "\n",
    "# create actual IDs\n",
    "actual = np.asarray(actual).argmax(axis=1)\n",
    "\n",
    "# make sure predicted and actual are the same size and shape\n",
    "print(predicted.shape)\n",
    "print(actual.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(\"[INFO] evaluating network...\")\n",
    "print(classification_report(actual, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can determine the confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm=confusion_matrix(actual,predicted)\n",
    "\n",
    "def print_cm(cm, frm, to,abs_or_relative=0):\n",
    "    import seaborn as sns\n",
    "    import matplotlib.pylab as plt\n",
    "\n",
    "    cm = cm[frm:to+1,frm:to+1]\n",
    "    # create labels\n",
    "    x_axis_labels = np.arange(frm,to+1)\n",
    "    y_axis_labels = np.arange(frm,to+1)\n",
    "    \n",
    "    plt.xticks(rotation=45)\n",
    "    plt.yticks(rotation=-45)\n",
    "    \n",
    "    if(abs_or_relative==0):\n",
    "        sns.heatmap(cm, annot=True,xticklabels=x_axis_labels, yticklabels=y_axis_labels)\n",
    "    else:\n",
    "        sns.heatmap(cm/np.sum(cm), annot=True, \n",
    "           fmt='.2%', cmap='Blues',\n",
    "           xticklabels=x_axis_labels, yticklabels=y_axis_labels)\n",
    "\n",
    "print_cm(cm,1 ,20,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we already have actual and predicted \n",
    "\n",
    "# also see https://www.dlology.com/blog/simple-guide-on-how-to-generate-roc-plot-for-keras-classifier/\n",
    "# for micro-average ROC curves as well\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "\n",
    "#extract the actual labels from the test data\n",
    "Y_test = []\n",
    "for i in range(0,int(test_steps_per_epoch)):\n",
    "    Y_test.extend(np.array(test_data[i][1]))\n",
    "Y_test = np.array(Y_test)\n",
    "n_classes = Y_test.shape[1]  # one hot encoded\n",
    "\n",
    "# create actual output from the model using test_data\n",
    "y_score=model.predict(x=test_data, steps=test_steps_per_epoch)\n",
    "\n",
    "print(Y_test.shape)\n",
    "print(y_score.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(n_classes)\n",
    "# compare each class's probabilities one by one\n",
    "# each acts like a single column\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(Y_test[:,i], y_score[:,i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# Print the AUC scores\n",
    "from IPython.display import display\n",
    "import pandas as pd\n",
    "auc_array = np.array(list(roc_auc.items()))\n",
    "df = pd.DataFrame(auc_array[:,1])\n",
    "df.columns = ['AUC']\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the ROC for the ith class cls\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "def plot_roc(cls,roc_dir):  \n",
    "    plt.plot(fpr[cls], tpr[cls], lw=2,label='ROC curve of class {0} (area = {1:0.3f})'\n",
    "    ''.format(cls, roc_auc[cls]))\n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(roc_dir, f'ROC_{cls}.png'))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# make sure directory exists\n",
    "def make_directory(roc_dir):\n",
    "    try:\n",
    "        os.mkdir(roc_dir)\n",
    "    except OSError:\n",
    "        print (\"Creation of the directory %s failed\" % roc_dir)\n",
    "    else:\n",
    "        print (\"Successfully created the directory %s \" % roc_dir)\n",
    "        \n",
    "# print the roc curve for 0\n",
    "\n",
    "make_directory('rocs')\n",
    "\n",
    "for i in range(n_classes):\n",
    "    plot_roc(i,'rocs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using tensorflow extension\n",
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define tensorboard callback\n",
    "\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "# Using remote tensorboard\n",
    "#https://blog.yyliu.net/remote-tensorboard/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "from keras import metrics\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer='adam', \n",
    "              metrics=['accuracy',\n",
    "                        metrics.AUC(name='auc'),\n",
    "                        metrics.Precision(name='precision'),\n",
    "                        metrics.Recall(name='recall')])\n",
    "\n",
    "# Fitting the model with more metrics\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "history=model.fit(train_data,\n",
    "                  steps_per_epoch =train_steps_per_epoch, \n",
    "                  validation_data=valid_data,\n",
    "                  epochs=num_epochs,\n",
    "                  validation_steps=valid_steps_per_epoch,\n",
    "                  callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}