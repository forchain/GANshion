{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# COE59410- Generative Deep Learning\n",
    "\n",
    "## Homework 1\n",
    "\n",
    "> Imran A. Zualkernan\n",
    "\n",
    "The purpose of this homework is to learn how to use the GPU machines to run large models. In addition, build familiarity with the Keras framework and the associated tools.\n",
    "\n",
    "## Deliverables\n",
    "\n",
    "**You need to upload the following on ilearn (2 items)**\n",
    "\n",
    "1. The Jupyter notebook in its original format.\n",
    "\n",
    "2. A PDF of the Jupyter notebook for grading.\n",
    "\n",
    "*Please do not upload a zipped file.* Upload each file separately. Each question is worth 25 points.\n",
    "\n",
    "* Q1. Load and run the large_scale_processing v1.1 Jupyter notebook on the GPU machine and show how you can use tensorboard to monitor the runs remotely on your local machine.\n",
    "\n",
    "* Q2. Modify the model in large_scale_processing v1.1 so that rather than a CNN, the model is a fully connected feedforward neural network. Fine tune the model to show your best results. Report and discuss all the results that are necessary to determine the goodness of your best model.\n",
    "\n",
    "> Hint: Use the Reshape Layer in Keras.\n",
    "\n",
    "* Q3. Use the following two call-backs on your best fully connected model and determine if you are able to improve the results. Clearly explain why or why not.\n",
    "    1. LearningRateScheduler\n",
    "    2. ReduceLROnPlateau\n",
    "\n",
    "* Q4. Use the Keras Hypertune and Random optimizers (https://keras-team.github.io/keras-tuner/) to determine if you can improve the model by varying the number of layers, neurons in each layer and the learning rate.\n",
    "\n",
    "    1. Plot the precision vs. recall of the best 20 models in one figure.\n",
    "    2. Show a complete evaluation of the top two models.\n",
    "\n",
    "## Group 2\n",
    "* Huangjin Zhou, b00080932\n",
    "* Eman Shaikh, G00086260\n",
    "* Mueez Kan, b00068255"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Useful links\n",
    "# https://www.hostinger.com/tutorials/ssh/basic-ssh-commands"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# print(tf.config.list_physical_devices('GPU'))\n",
    "tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.list_physical_devices(device_type=None)\n",
    "tf.test.is_gpu_available()\n",
    "print(tf.test.is_built_with_cuda())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image, display\n",
    "import random\n",
    "import math\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Model, Sequential\n",
    "from keras.utils import plot_model\n",
    "from keras.layers import Reshape, Input, Dense, Dropout, Flatten, Activation,Concatenate\n",
    "from keras.layers import Conv2D, MaxPooling2D, AveragePooling2D\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend, models\n",
    "#import tensorflow_addons as tfa\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n",
    "# need to add these for the GPU\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.compat.v1.Session(config=config)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# import the image generator\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Setting the parameters for training\n",
    "\n",
    "# batch size and image width to use\n",
    "batch_size=128\n",
    "width=100\n",
    "\n",
    "# all the data directories\n",
    "train_dir='train/'\n",
    "test_dir='test/'\n",
    "valid_dir='valid/'\n",
    "\n",
    "# the number of epochs\n",
    "num_epochs=10\n",
    "\n",
    "# creating an image generator that will feed the data from\n",
    "# each of the directories\n",
    "\n",
    "# we use scaling transformation in this generator\n",
    "generator=ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# we specify the size of the input and batch size\n",
    "# size of the input is necessary because the image\n",
    "# needs to be rescaled for the neural network\n",
    "\n",
    "train_data=generator.flow_from_directory(train_dir, target_size=(width,width),batch_size=batch_size)\n",
    "valid_data=generator.flow_from_directory(valid_dir, target_size=(width,width),batch_size=batch_size)\n",
    "test_data=generator.flow_from_directory(test_dir, target_size=(width,width),batch_size=batch_size)\n",
    "\n",
    "# the number of steps per epoch is samples/batch size\n",
    "# we need to use these numbers later\n",
    "\n",
    "train_steps_per_epoch=math.ceil(train_data.samples/batch_size)\n",
    "valid_steps_per_epoch=math.ceil(valid_data.samples/batch_size)\n",
    "test_steps_per_epoch=math.ceil(test_data.samples/batch_size)\n",
    "print(train_steps_per_epoch)\n",
    "print(valid_steps_per_epoch)\n",
    "print(test_steps_per_epoch)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "> Q1. Load and run the large_scale_processing v1.1 Jupyter notebook on the GPU machine and\n",
    "show how you can use tensorboard to monitor the runs remotely on your local machine.\n",
    "\n",
    "\n",
    "```shell\n",
    "$ jupyter notebook --port 9999 --NotebookApp.allow_remote_access=True\n",
    "$ tensorboard --logdir logs/fit --port=8888\n",
    "```\n",
    "> Scalars\n",
    "![Scalars](images/scalar.jpg)\n",
    "![Graph](images/graph.jpg)\n",
    "![Time Series](images/timeseries.jpg)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Q1 = False\n",
    "if Q1:\n",
    "    # the actual model should go here\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), input_shape=(width, width, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Conv2D(256, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1024, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(250, activation='softmax'))\n",
    "    model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if Q1:\n",
    "    # Compile the model\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "    # see if the model is good.\n",
    "    print(model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if Q1:\n",
    "    print(valid_steps_per_epoch)\n",
    "    num_epochs = 20\n",
    "    # fit model and add tensor board callbacks\n",
    "    from tensorflow.keras.callbacks import TensorBoard\n",
    "    tensorboard = TensorBoard(log_dir='logs/fit')\n",
    "    history=model.fit(train_data,\n",
    "                  steps_per_epoch =train_steps_per_epoch,\n",
    "                  validation_data=valid_data,\n",
    "                  epochs=num_epochs,\n",
    "                  validation_steps=valid_steps_per_epoch, callbacks=[tensorboard])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "Q2. Modify the model in large_scale_processing v1.1 so that rather than a CNN,\n",
    "the model is a fully connected feedforward neural network.\n",
    "Fine tune the model to show your best results.\n",
    "Report and discuss all the results that are necessary to determine the goodness of your best model.\n",
    "\n",
    "\n",
    "> Hint: Use the Reshape Layer in Keras.\n",
    "\n",
    "### adjust hidden units\n",
    "we fix only 1 hidden layer and to see which unit number is best for us\n",
    "\n",
    "```python\n",
    "hidden_units = 128*1\n",
    "```\n",
    "```shell\n",
    " loss: 5.5032 - accuracy: 0.0087 - val_loss: 5.5339 - val_accuracy: 0.0040\n",
    "```\n",
    "\n",
    "```python\n",
    "hidden_units = 128*2\n",
    "```\n",
    "```shell\n",
    " loss: 5.5122 - accuracy: 0.0076 - val_loss: 5.5246 - val_accuracy: 0.0040\n",
    "```\n",
    "\n",
    "```python\n",
    "hidden_units = 128*4\n",
    "```\n",
    "```shell\n",
    " loss: 5.3341 - accuracy: 0.0126 - val_loss: 5.3179 - val_accuracy: 0.0120\n",
    "```\n",
    "```python\n",
    "hidden_units = 128*8\n",
    "```\n",
    "```shell\n",
    " loss: 4.8888 - accuracy: 0.0391 - val_loss: 4.7778 - val_accuracy: 0.0336\n",
    "```\n",
    "```python\n",
    "hidden_units = 128*16\n",
    "```\n",
    "```shell\n",
    " loss: 4.6924 - accuracy: 0.0555 - val_loss: 4.6376 - val_accuracy: 0.0520\n",
    "```\n",
    "\n",
    "```python\n",
    "hidden_units = 128*32\n",
    "```\n",
    "```shell\n",
    " loss: 4.9463 - accuracy: 0.0294 - val_loss: 4.8971 - val_accuracy: 0.0312\n",
    "```\n",
    "\n",
    "from the report, we could make a conclusion\n",
    "1. the val_accuracy on 128*32=4096 units is lower than on 128*16=2048, that is 0.0312 < 0.0312, we stop try more units.\n",
    "2. the accuracy increased ratio from 1024 units to 2048 , which is 0.0520/0.0336=1.54, is less than that from 512 units to 1024,\n",
    "which is 0.0336/0.0120=2.8. Therefore, if we have enough computation resources, we prefer 2048, otherwise 1024.\n",
    "\n",
    "### adjust hidden layers\n",
    "\n",
    "we fixed units number to 1024 or 2048, and then increase the layer number to see how many\n",
    "layers work best\n",
    "\n",
    "```python\n",
    "hidden_units = 128*8\n",
    "\n",
    "model.add(Dense(hidden_units, activation='relu'))\n",
    "model.add(Dense(hidden_units, activation='relu'))\n",
    "...\n",
    "```\n",
    "```shell\n",
    "loss: 4.2517 - accuracy: 0.0861 - val_loss: 4.1615 - val_accuracy: 0.0952\n",
    "```\n",
    "\n",
    "\n",
    "```python\n",
    "hidden_units = 128*16\n",
    "model.add(Dense(hidden_units, activation='relu'))\n",
    "model.add(Dense(hidden_units, activation='relu'))\n",
    "...\n",
    "```\n",
    "```shell\n",
    "loss: 5.5034 - accuracy: 0.0085 - val_loss: 5.5354 - val_accuracy: 0.0040\n",
    "```\n",
    "stop trying 128*16 units for one layer since accuracy doesn't increase\n",
    "\n",
    "```python\n",
    "hidden_units = 128*8\n",
    "model.add(Dense(hidden_units, activation='relu'))\n",
    "model.add(Dense(hidden_units, activation='relu'))\n",
    "model.add(Dense(hidden_units, activation='relu'))\n",
    "```\n",
    "```shell\n",
    " loss: 3.1438 - accuracy: 0.2647 - val_loss: 3.4928 - val_accuracy: 0.2296\n",
    "```\n",
    "\n",
    "```python\n",
    "hidden_units = 128*8\n",
    "model.add(Dense(hidden_units, activation='relu'))\n",
    "model.add(Dense(hidden_units, activation='relu'))\n",
    "model.add(Dense(hidden_units, activation='relu'))\n",
    "model.add(Dense(hidden_units, activation='relu'))\n",
    "```\n",
    "```shell\n",
    " loss: 3.2843 - accuracy: 0.2326 - val_loss: 3.5376 - val_accuracy: 0.2080\n",
    "```\n",
    "\n",
    "##### conclusion\n",
    "1. units of 2048 for single layer is too much, because the accuracy of two dense layer goes\n",
    "lower than that of single layer, which stops at 0.0040\n",
    "2. 3 dense layers are best match with 1024 units number, since the accuracy of 4 layers\n",
    "goes lower than that of 3 layers, which is 0.2080 < 0.2296\n",
    "\n",
    "### try more epochs\n",
    "\n",
    "we set epoch number to 10 just to find hyperparameters quickly, however, this epoch is\n",
    "not enough since the accuracy keeps going up, we will enlarge this number and observe\n",
    "the accuracy until it converges\n",
    "\n",
    "```python\n",
    "num_epochs = 10*10\n",
    "\n",
    "hidden_units = 128*8\n",
    "\n",
    "model.add(Dense(hidden_units, activation='relu'))\n",
    "model.add(Dense(hidden_units, activation='relu'))\n",
    "model.add(Dense(hidden_units, activation='relu'))\n",
    "```\n",
    "```shell\n",
    "Epoch 20/100\n",
    "loss: 1.3170 - accuracy: 0.6387 - val_loss: 4.6068 - val_accuracy: 0.2816\n",
    "...\n",
    "Epoch 30/100\n",
    "loss: 0.4543 - accuracy: 0.8656 - val_loss: 7.4723 - val_accuracy: 0.2720\n",
    "```\n",
    "\n",
    "conclusion:\n",
    "the accuracy reaches to top of 0.2816 at epoch 20, so we assume 20 epochs is enough\n",
    "\n",
    "we sum up, hidden units, layer number, epoch number, and the final code is as below:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Q2 = False\n",
    "Q3 = False\n",
    "if Q2 or Q3:\n",
    "    num_epochs = 20\n",
    "\n",
    "    # try different units\n",
    "\n",
    "    num_labels = train_data.num_classes\n",
    "\n",
    "    hidden_units = 1024\n",
    "    model = Sequential()\n",
    "    model.add(Reshape((-1,), input_shape=(width, width, 3)))\n",
    "    model.add(Dense(hidden_units, activation='relu'))\n",
    "    model.add(Dense(hidden_units, activation='relu'))\n",
    "    model.add(Dense(hidden_units, activation='relu'))\n",
    "    model.add(Dense(num_labels, activation='softmax'))\n",
    "    model.summary()\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    # see if the model is good.\n",
    "    print(model)\n",
    "\n",
    "if Q2 :\n",
    "    history=model.fit(train_data,\n",
    "                      steps_per_epoch =train_steps_per_epoch,\n",
    "                      validation_data=valid_data,\n",
    "                      epochs=num_epochs,\n",
    "                      validation_steps=valid_steps_per_epoch)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Q3. Use the following two call-backs on your best fully connected model and determine if you are able to\n",
    "improve the results. Clearly explain why or why not.\n",
    "    1. LearningRateScheduler\n",
    "    2. ReduceLROnPlateau\n",
    "\n",
    "### LearningRateScheduler\n",
    "from the accuracy of Q2, we would see the accuracy drops from 0.2128 to 0.1944 within epochs 6 to 8, this drop is\n",
    "probably because learning rate is too big, so try to tune LR to increase the accuracy\n",
    "\n",
    "so we have two directions to find the best learning rate as follows, we take the outcome from Q2 as baseline,\n",
    "and we only try 5 epoch for efficiency\n",
    "\n",
    "#### baseline\n",
    "```\n",
    "Epoch 5/20\n",
    "loss: 3.9027 - accuracy: 0.1498 - val_loss: 3.7048 - val_accuracy: 0.1664\n",
    "```\n",
    "\n",
    "#### find the best consistent learning rate\n",
    "we tried a fixed LR from 1e-1 to 1e-10 to see which one is the best\n",
    "\n",
    "```python\n",
    "    lr = 1e-1\n",
    "```\n",
    "```\n",
    " loss: 5.5387 - accuracy: 0.0057 - val_loss: 5.5664 - val_accuracy: 0.0040\n",
    "```\n",
    "\n",
    "```python\n",
    "    lr = 1e-2\n",
    "```\n",
    "```\n",
    "  loss: 4.8003 - accuracy: 0.0375 - val_loss: 4.6560 - val_accuracy: 0.0424\n",
    "```\n",
    "\n",
    "```python\n",
    "    lr = 1e-3\n",
    "```\n",
    "```\n",
    " loss: 3.8051 - accuracy: 0.1694 - val_loss: 3.7018 - val_accuracy: 0.1792\n",
    "```\n",
    "\n",
    "```python\n",
    "    lr = 1e-4\n",
    "```\n",
    "```\n",
    " loss: 3.8836 - accuracy: 0.1757 - val_loss: 3.7352 - val_accuracy: 0.1816\n",
    "```\n",
    "\n",
    "```python\n",
    "    lr = 1e-5\n",
    "```\n",
    "```\n",
    " loss: 4.6433 - accuracy: 0.0939 - val_loss: 4.5639 - val_accuracy: 0.0976\n",
    "```\n",
    "\n",
    "##### Conclusion\n",
    "The accuracy goes lower when LR equals 1e-5, so the best LR would be within 1e-4 and 1e-5\n",
    "\n",
    "#### find the best dynamic learning rate\n",
    "since the best LR would be within 1e-4 and 1e-5, there must be a turning point at a specific epoch.\n",
    "we run a 20-epoch experiment to find this turning point and make LR below it use LR=1e-4 and LR=1e-5\n",
    "for the rest.\n",
    "\n",
    "\n",
    "\n",
    "for LR=1e-4\n",
    "```python\n",
    "    num_epochs = 20\n",
    "    lr = 1e-4\n",
    "```\n",
    "```shell\n",
    "Epoch 10/20\n",
    "poch 9/20\n",
    " loss: 3.2539 - accuracy: 0.2798 - val_loss: 3.2876 - val_accuracy: 0.2592\n",
    "Epoch 10/20\n",
    " loss: 3.1296 - accuracy: 0.3019 - val_loss: 3.2010 - val_accuracy: 0.2776\n",
    "Epoch 11/20\n",
    " loss: 2.9819 - accuracy: 0.3284 - val_loss: 3.2834 - val_accuracy: 0.2552\n",
    "Epoch 12/20\n",
    "```\n",
    "we could see the turning point was at Epoch 10.\n",
    "\n",
    "for LR=1e-4\n",
    "the general performance is better than 1e-4, and even we get a new high at 33.52% accuracy\n",
    "\n",
    "```shell\n",
    "Epoch 19/20\n",
    " loss: 2.1844 - accuracy: 0.4862 - val_loss: 2.9645 - val_accuracy: 0.3352\n",
    "```\n",
    "\n",
    "So we could make 1e-4 as the LR for the first 10 epochs and 1e-5 for the rest\n",
    "\n",
    "```python\n",
    "    # starting point\n",
    "    lr = 1e-4\n",
    "    # dynamic\n",
    "    if epoch > 10:\n",
    "        lr = 1e-5\n",
    "```\n",
    "however, the max accuracy is only 0.3064, lower than consistent LR of 1e-4, which is 33.52%\n",
    "```shell\n",
    "Epoch 20/20\n",
    " loss: 2.6335 - accuracy: 0.4074 - val_loss: 2.9782 - val_accuracy: 0.3064\n",
    "```\n",
    "\n",
    "\n",
    "#### Conclusion\n",
    "The possible reason is that for the gradient descending algorithm, we could only find local optimization,\n",
    "not the global one. So, the epoch 10 drops in local optimization point.\n",
    "\n",
    "\n",
    "So, we will drop this dynamic LR optimization and stick to the previous one, which is that the LR is a consistent\n",
    "number 1e-4.\n",
    "\n",
    "\n",
    "### ReduceLROnPlateau\n",
    "Now we could add a reducer, since we don't find an apparent plateau in the 20 epochs, we tried 100 epoch to see if it works.\n",
    "The min LR is 1e-6, 1% of the basic LR 1e-4 is reasonable from our aspect.\n",
    "\n",
    "\n",
    "```python\n",
    "num_epochs = 10*10\n",
    "\n",
    "lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1),\n",
    "                               cooldown=0,\n",
    "                               patience=5,\n",
    "                               min_lr=1e-6)\n",
    "```\n",
    "\n",
    "Since the patience is 5, which means the learning rate will be reduced once there is no improvement in 5 epochs.\n",
    "It works as expected, take a look at the first example.\n",
    "\n",
    "Epoch 22 is a turning point, so we would expect the accuracy of epoch 24 is highly likely to be lower than that of\n",
    "epoch 23, however, after ajudsting the learning rate, accuracy of Epoch 24 goes up as a new top, which is 0.3544\n",
    "\n",
    "```shell\n",
    "Epoch 21/100\n",
    " loss: 1.9319 - accuracy: 0.5408 - val_loss: 2.9627 - val_accuracy: 0.3440\n",
    "Epoch 22/100\n",
    " loss: 1.8510 - accuracy: 0.5583 - val_loss: 2.9558 - val_accuracy: 0.3488\n",
    "Epoch 23/100\n",
    " loss: 1.7744 - accuracy: 0.5757 - val_loss: 3.0351 - val_accuracy: 0.3296\n",
    "\n",
    "Epoch 00023: ReduceLROnPlateau reducing learning rate to 3.1622775802825264e-05.\n",
    "Epoch 24/100\n",
    " loss: 1.6634 - accuracy: 0.6005 - val_loss: 2.9863 - val_accuracy: 0.3544\n",
    "```\n",
    "\n",
    "This optimization did a great job, we break a new record at epoch 86, which is **0.3928**\n",
    "```\n",
    "Epoch 86/100\n",
    "loss: 0.0059 - accuracy: 0.9999 - val_loss: 5.4967 - val_accuracy: 0.3928\n",
    "```\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if Q3:\n",
    "    from tensorflow.keras.callbacks import LearningRateScheduler, ReduceLROnPlateau\n",
    "\n",
    "\n",
    "    # static\n",
    "    def lr_schedule(epoch):\n",
    "        lr = 1e-4\n",
    "\n",
    "        # dynamic optimization, abandoned due to bad performance\n",
    "        # if epoch > 10:\n",
    "        #     lr = 1e-5\n",
    "\n",
    "        return lr\n",
    "\n",
    "    lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "\n",
    "    lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1),\n",
    "                                   cooldown=0,\n",
    "                                   patience=5,\n",
    "                                   min_lr=1e-6)\n",
    "\n",
    "    callbacks = [lr_reducer, lr_scheduler]\n",
    "    history=model.fit(train_data,\n",
    "                  steps_per_epoch =train_steps_per_epoch,\n",
    "                  validation_data=valid_data,\n",
    "                  epochs=num_epochs,\n",
    "                  validation_steps=valid_steps_per_epoch, callbacks=callbacks)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Q4. Use the Keras Hypertune and Random optimizers (https://keras-team.github.io/keras-tuner/) to determine\n",
    "if you can improve the model by varying the number of layers, neurons in each layer and the learning rate.\n",
    "\n",
    "1. Plot the precision vs. recall of the best 20 models in one figure.\n",
    "2. Show a complete evaluation of the top two models.\n",
    "\n",
    "\n",
    "first, we need to install keras-tuner\n",
    "```shell\n",
    "$ conda install -c conda-forge keras-tuner\n",
    "\n",
    "```\n",
    "\n",
    "### Random Search\n",
    "1. Since the best units number we have found manually is 1024, within 512 and 2048, so we fill the two\n",
    "with min and max value\n",
    "2. Since the best learning rate we've found manually is , within 1e-3 and 1e-5, so we will try\n",
    "1e-3, 1e-4, 1e-5\n",
    "3. To improve efficiency, we just try 3 epochs for each combination\n",
    "4. We fit the best 20 models with 5 epochs to plot the chart and 10 epochs to evaluate the top 2.\n",
    "\n",
    "### Hyper Band\n",
    "We compared the general performance between Random Search and Hyper Band. Unexpectedly, the Hyper Band worked much worse than\n",
    "Random Search, so we didn't try Hyper Band anymore, all the outcomes come from Random Search.\n",
    "\n",
    "\n",
    "The score is only 0.004\n",
    "```\n",
    "Results summary\n",
    "|-Results in my_dir/hyperband\n",
    "|-Showing 10 best trials\n",
    "|-Objective(name='val_accuracy', direction='max')\n",
    "Trial summary\n",
    "|-Trial ID: 1f24cc1576561684c9e3f8ef94220c8b\n",
    "|-Score: 0.004000000189989805\n",
    "|-Best step: 0\n",
    "Hyperparameters:\n",
    "|-conv3_depth: 4\n",
    "|-conv4_depth: 36\n",
    "|-learning_rate: 0.001\n",
    "|-optimizer: adam\n",
    "|-pooling: max\n",
    "|-tuner/bracket: 0\n",
    "|-tuner/epochs: 1\n",
    "|-tuner/initial_epoch: 0\n",
    "|-tuner/round: 0\n",
    "|-version: v2\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Q4 = True\n",
    "from keras import backend as K\n",
    "from keras import metrics\n",
    "\n",
    "def plot_p_r(m, epochs):\n",
    "    h=m.fit(train_data,\n",
    "                  steps_per_epoch =train_steps_per_epoch,\n",
    "                  validation_data=valid_data,\n",
    "                  epochs=epochs,\n",
    "                  validation_steps=valid_steps_per_epoch\n",
    "                  )\n",
    "    # Plot accuracy vs. prevision\n",
    "    plt.plot(h.history['precision'],h.history['recall'],'o', color='black',label='precision vs. recall')\n",
    "    plt.plot(h.history['val_precision'],h.history['val_recall'],'o', color='red',label='val_precision vs. val_recall')\n",
    "    plt.title('Model Precision and Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def evaluate(m):\n",
    "    # Evaluate against test data.\n",
    "    scores = m.evaluate(test_data, verbose=1)\n",
    "\n",
    "    print('Test loss:', scores[0])\n",
    "    print('Test accuracy:', scores[1])\n",
    "    print('Test AUC:', scores[2])\n",
    "    print('Test precision:', scores[3])\n",
    "    print('Test recall:', scores[4])\n",
    "    print('Test F1-Score:', scores[5])\n",
    "\n",
    "def F1_Score(y_true, y_pred): #taken from old keras source code\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n",
    "    return f1_val\n",
    "\n",
    "num_labels = train_data.num_classes\n",
    "tries = 30\n",
    "num_epochs = 3\n",
    "from kerastuner.tuners import RandomSearch\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "    model.add(Reshape((-1,), input_shape=(width, width, 3)))\n",
    "    model.add(Dense(units=hp.Int('units',\n",
    "                                        min_value=512,\n",
    "                                        max_value=2048,\n",
    "                                        step=128),\n",
    "                           activation='relu'))\n",
    "    model.add(Dense(units=hp.Int('units',\n",
    "                                        min_value=512,\n",
    "                                        max_value=2048,\n",
    "                                        step=128),\n",
    "                           activation='relu'))\n",
    "    model.add(Dense(units=hp.Int('units',\n",
    "                                        min_value=512,\n",
    "                                        max_value=2048,\n",
    "                                        step=128),\n",
    "                           activation='relu'))\n",
    "    model.add(Dense(num_labels, activation='softmax'))\n",
    "    model.compile(\n",
    "        optimizer=Adam(\n",
    "            hp.Choice('learning_rate',\n",
    "                      values=[1e-3, 1e-4, 1e-5])),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy',\n",
    "                   metrics.AUC(name='auc'),\n",
    "                   metrics.Precision(name='precision'),\n",
    "                   metrics.Recall(name='recall'),\n",
    "                   F1_Score]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def random_search():\n",
    "    tuner = RandomSearch(\n",
    "        build_model,\n",
    "        objective='val_accuracy',\n",
    "        max_trials=tries,\n",
    "        executions_per_trial=1,\n",
    "        directory='my_dir',\n",
    "        project_name='randomsearch')\n",
    "\n",
    "    tuner.search_space_summary()\n",
    "\n",
    "    tuner.search(train_data,\n",
    "                  steps_per_epoch =train_steps_per_epoch,\n",
    "                  validation_data=valid_data,\n",
    "                  epochs=num_epochs,\n",
    "                  validation_steps=valid_steps_per_epoch)\n",
    "    tuner.results_summary()\n",
    "\n",
    "    return tuner.get_best_models(20)\n",
    "\n",
    "random_models =  random_search()\n",
    "\n",
    "def hyper_band():\n",
    "    from kerastuner.applications import HyperResNet\n",
    "    from kerastuner.tuners import Hyperband\n",
    "\n",
    "    hypermodel = HyperResNet(input_shape=(width, width, 3), classes=num_labels)\n",
    "\n",
    "    tuner = Hyperband(\n",
    "        hypermodel,\n",
    "        objective='val_accuracy',\n",
    "        max_epochs=10,\n",
    "        metrics=['accuracy',\n",
    "                   metrics.AUC(name='auc'),\n",
    "                   metrics.Precision(name='precision'),\n",
    "                   metrics.Recall(name='recall'),\n",
    "                   F1_Score],\n",
    "        directory='my_dir',\n",
    "        project_name='hyperband')\n",
    "\n",
    "    tuner.search(train_data,\n",
    "                  steps_per_epoch =train_steps_per_epoch,\n",
    "                  validation_data=valid_data,\n",
    "                  epochs=num_epochs,\n",
    "                  validation_steps=valid_steps_per_epoch)\n",
    "\n",
    "    tuner.results_summary()\n",
    "    return tuner.get_best_models(tries)\n",
    "\n",
    "#hyper_models = hyper_band()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for m in random_models:\n",
    "    plot_p_r(m, 5)\n",
    "\n",
    "evaluate(random_models[0])\n",
    "evaluate(random_models[1])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}