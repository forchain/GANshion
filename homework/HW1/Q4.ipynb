{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COE59410- Generative Deep Learning\n",
    "\n",
    "## Homework 1\n",
    "\n",
    "> Imran A. Zualkernan\n",
    "\n",
    "The purpose of this homework is to learn how to use the GPU machines to run large models. In addition, build familiarity with the Keras framework and the associated tools.\n",
    "\n",
    "## Deliverables\n",
    "\n",
    "**You need to upload the following on ilearn (2 items)**\n",
    "\n",
    "1. The Jupyter notebook in its original format.\n",
    "\n",
    "2. A PDF of the Jupyter notebook for grading.\n",
    "\n",
    "*Please do not upload a zipped file.* Upload each file separately. Each question is worth 25 points.\n",
    "\n",
    "* Q1. Load and run the large_scale_processing v1.1 Jupyter notebook on the GPU machine and show how you can use tensorboard to monitor the runs remotely on your local machine.\n",
    "\n",
    "* Q2. Modify the model in large_scale_processing v1.1 so that rather than a CNN, the model is a fully connected feedforward neural network. Fine tune the model to show your best results. Report and discuss all the results that are necessary to determine the goodness of your best model.\n",
    "\n",
    "> Hint: Use the Reshape Layer in Keras.\n",
    "\n",
    "* Q3. Use the following two call-backs on your best fully connected model and determine if you are able to improve the results. Clearly explain why or why not.\n",
    "    1. LearningRateScheduler\n",
    "    2. ReduceLROnPlateau\n",
    "\n",
    "* Q4. Use the Keras Hypertune and Random optimizers (https://keras-team.github.io/keras-tuner/) to determine if you can improve the model by varying the number of layers, neurons in each layer and the learning rate.\n",
    "\n",
    "    1. Plot the precision vs. recall of the best 20 models in one figure.\n",
    "    2. Show a complete evaluation of the top two models.\n",
    "\n",
    "## Group 2\n",
    "* Eman ,\n",
    "* Huangjin Zhou, b00080932\n",
    "* Mueez ,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful links \n",
    "# https://www.hostinger.com/tutorials/ssh/basic-ssh-commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 12684984285302164433\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 161415168\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 1337013905726444326\n",
      "physical_device_desc: \"device: 0, name: Quadro RTX 4000, pci bus id: 0000:01:00.0, compute capability: 7.5\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-3-78f31cea5abc>:5: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "# print(tf.config.list_physical_devices('GPU'))\n",
    "tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.list_physical_devices(device_type=None)\n",
    "tf.test.is_gpu_available()\n",
    "print(tf.test.is_built_with_cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image, display\n",
    "import random\n",
    "import math\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Model, Sequential\n",
    "from keras.utils import plot_model \n",
    "from keras.layers import Reshape, Input, Dense, Dropout, Flatten, Activation,Concatenate\n",
    "from keras.layers import Conv2D, MaxPooling2D, AveragePooling2D\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend, models\n",
    "#import tensorflow_addons as tfa\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n",
    "# need to add these for the GPU\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.compat.v1.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the image generator\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 35215 images belonging to 250 classes.\n",
      "Found 1250 images belonging to 250 classes.\n",
      "Found 1250 images belonging to 250 classes.\n",
      "276\n",
      "10\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "#Setting the parameters for training\n",
    "\n",
    "# batch size and image width to use\n",
    "batch_size=128\n",
    "width=100\n",
    "\n",
    "# all the data directories\n",
    "train_dir='train/'\n",
    "test_dir='test/'\n",
    "valid_dir='valid/'\n",
    "\n",
    "# the number of epochs\n",
    "num_epochs=10\n",
    "\n",
    "# creating an image generator that will feed the data from\n",
    "# each of the directories\n",
    "\n",
    "# we use scaling transformation in this generator\n",
    "generator=ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# we specify the size of the input and batch size\n",
    "# size of the input is necessary because the image\n",
    "# needs to be rescaled for the neural network\n",
    "\n",
    "train_data=generator.flow_from_directory(train_dir, target_size=(width,width),batch_size=batch_size)\n",
    "valid_data=generator.flow_from_directory(valid_dir, target_size=(width,width),batch_size=batch_size)\n",
    "test_data=generator.flow_from_directory(test_dir, target_size=(width,width),batch_size=batch_size)\n",
    "\n",
    "# the number of steps per epoch is samples/batch size\n",
    "# we need to use these numbers later\n",
    "\n",
    "train_steps_per_epoch=math.ceil(train_data.samples/batch_size)\n",
    "valid_steps_per_epoch=math.ceil(valid_data.samples/batch_size)\n",
    "test_steps_per_epoch=math.ceil(test_data.samples/batch_size)\n",
    "print(train_steps_per_epoch)\n",
    "print(valid_steps_per_epoch)\n",
    "print(test_steps_per_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "> Q1. Load and run the large_scale_processing v1.1 Jupyter notebook on the GPU machine and\n",
    "show how you can use tensorboard to monitor the runs remotely on your local machine.\n",
    "\n",
    "\n",
    "```shell\n",
    "$ jupyter notebook --port 9999 --NotebookApp.allow_remote_access=True\n",
    "$ tensorboard --logdir logs/fit --port=8888\n",
    "```\n",
    "> Scalars\n",
    "![Scalars](images/scalar.jpg)\n",
    "![Graph](images/graph.jpg)\n",
    "![Time Series](images/timeseries.jpg)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1 = False\n",
    "if Q1:\n",
    "    # the actual model should go here\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), input_shape=(width, width, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Conv2D(256, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1024, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(250, activation='softmax'))\n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "if Q1:\n",
    "    # Compile the model\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "    # see if the model is good.\n",
    "    print(model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "if Q1:\n",
    "    print(valid_steps_per_epoch)\n",
    "    num_epochs = 20\n",
    "    # fit model and add tensor board callbacks\n",
    "    from tensorflow.keras.callbacks import TensorBoard\n",
    "    tensorboard = TensorBoard(log_dir='logs/fit')\n",
    "    history=model.fit(train_data,\n",
    "                  steps_per_epoch =train_steps_per_epoch,\n",
    "                  validation_data=valid_data,\n",
    "                  epochs=num_epochs,\n",
    "                  validation_steps=valid_steps_per_epoch, callbacks=[tensorboard])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "Q2. Modify the model in large_scale_processing v1.1 so that rather than a CNN,\n",
    "the model is a fully connected feedforward neural network.\n",
    "Fine tune the model to show your best results.\n",
    "Report and discuss all the results that are necessary to determine the goodness of your best model.\n",
    "\n",
    "\n",
    "> Hint: Use the Reshape Layer in Keras.\n",
    "\n",
    "### adjust hidden units\n",
    "we fix only 1 hidden layer and to see which unit number is best for us\n",
    "\n",
    "```python\n",
    "hidden_units = 128*1\n",
    "```\n",
    "```shell\n",
    " loss: 5.5032 - accuracy: 0.0087 - val_loss: 5.5339 - val_accuracy: 0.0040\n",
    "```\n",
    "\n",
    "```python\n",
    "hidden_units = 128*2\n",
    "```\n",
    "```shell\n",
    " loss: 5.5122 - accuracy: 0.0076 - val_loss: 5.5246 - val_accuracy: 0.0040\n",
    "```\n",
    "\n",
    "```python\n",
    "hidden_units = 128*4\n",
    "```\n",
    "```shell\n",
    " loss: 5.3341 - accuracy: 0.0126 - val_loss: 5.3179 - val_accuracy: 0.0120\n",
    "```\n",
    "```python\n",
    "hidden_units = 128*8\n",
    "```\n",
    "```shell\n",
    " loss: 4.8888 - accuracy: 0.0391 - val_loss: 4.7778 - val_accuracy: 0.0336\n",
    "```\n",
    "```python\n",
    "hidden_units = 128*16\n",
    "```\n",
    "```shell\n",
    " loss: 4.6924 - accuracy: 0.0555 - val_loss: 4.6376 - val_accuracy: 0.0520\n",
    "```\n",
    "\n",
    "```python\n",
    "hidden_units = 128*32\n",
    "```\n",
    "```shell\n",
    " loss: 4.9463 - accuracy: 0.0294 - val_loss: 4.8971 - val_accuracy: 0.0312\n",
    "```\n",
    "\n",
    "from the report, we could make a conclusion\n",
    "1. the val_accuracy on 128*32=4096 units is lower than on 128*16=2048, that is 0.0312 < 0.0312, we stop try more units.\n",
    "2. the accuracy increased ratio from 1024 units to 2048 , which is 0.0520/0.0336=1.54, is less than that from 512 units to 1024,\n",
    "which is 0.0336/0.0120=2.8. Therefore, if we have enough computation resources, we prefer 2048, otherwise 1024.\n",
    "\n",
    "### adjust hidden layers\n",
    "\n",
    "we fixed units number to 1024 or 2048, and then increase the layer number to see how many\n",
    "layers work best\n",
    "\n",
    "```python\n",
    "hidden_units = 128*8\n",
    "\n",
    "model.add(Dense(hidden_units, activation='relu'))\n",
    "model.add(Dense(hidden_units, activation='relu'))\n",
    "...\n",
    "```\n",
    "```shell\n",
    "loss: 4.2517 - accuracy: 0.0861 - val_loss: 4.1615 - val_accuracy: 0.0952\n",
    "```\n",
    "\n",
    "\n",
    "```python\n",
    "hidden_units = 128*16\n",
    "model.add(Dense(hidden_units, activation='relu'))\n",
    "model.add(Dense(hidden_units, activation='relu'))\n",
    "...\n",
    "```\n",
    "```shell\n",
    "loss: 5.5034 - accuracy: 0.0085 - val_loss: 5.5354 - val_accuracy: 0.0040\n",
    "```\n",
    "stop trying 128*16 units for one layer since accuracy doesn't increase\n",
    "\n",
    "```python\n",
    "hidden_units = 128*8\n",
    "model.add(Dense(hidden_units, activation='relu'))\n",
    "model.add(Dense(hidden_units, activation='relu'))\n",
    "model.add(Dense(hidden_units, activation='relu'))\n",
    "```\n",
    "```shell\n",
    " loss: 3.1438 - accuracy: 0.2647 - val_loss: 3.4928 - val_accuracy: 0.2296\n",
    "```\n",
    "\n",
    "```python\n",
    "hidden_units = 128*8\n",
    "model.add(Dense(hidden_units, activation='relu'))\n",
    "model.add(Dense(hidden_units, activation='relu'))\n",
    "model.add(Dense(hidden_units, activation='relu'))\n",
    "model.add(Dense(hidden_units, activation='relu'))\n",
    "```\n",
    "```shell\n",
    " loss: 3.2843 - accuracy: 0.2326 - val_loss: 3.5376 - val_accuracy: 0.2080\n",
    "```\n",
    "\n",
    "##### conclusion\n",
    "1. units of 2048 for single layer is too much, because the accuracy of two dense layer goes\n",
    "lower than that of single layer, which stops at 0.0040\n",
    "2. 3 dense layers are best match with 1024 units number, since the accuracy of 4 layers\n",
    "goes lower than that of 3 layers, which is 0.2080 < 0.2296\n",
    "\n",
    "### try more epochs\n",
    "\n",
    "we set epoch number to 10 just to find hyperparameters quickly, however, this epoch is\n",
    "not enough since the accuracy keeps going up, we will enlarge this number and observe\n",
    "the accuracy until it converges\n",
    "\n",
    "```python\n",
    "num_epochs = 10*10\n",
    "\n",
    "hidden_units = 128*8\n",
    "\n",
    "model.add(Dense(hidden_units, activation='relu'))\n",
    "model.add(Dense(hidden_units, activation='relu'))\n",
    "model.add(Dense(hidden_units, activation='relu'))\n",
    "```\n",
    "```shell\n",
    "Epoch 20/100\n",
    "loss: 1.3170 - accuracy: 0.6387 - val_loss: 4.6068 - val_accuracy: 0.2816\n",
    "...\n",
    "Epoch 30/100\n",
    "loss: 0.4543 - accuracy: 0.8656 - val_loss: 7.4723 - val_accuracy: 0.2720\n",
    "```\n",
    "\n",
    "conclusion:\n",
    "the accuracy reaches to top of 0.2816 at epoch 20, so we assume 20 epochs is enough\n",
    "\n",
    "we sum up, hidden units, layer number, epoch number, and the final code is as below:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n",
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "reshape (Reshape)            (None, 30000)             0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1024)              30721024  \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 250)               256250    \n",
      "=================================================================\n",
      "Total params: 32,026,874\n",
      "Trainable params: 32,026,874\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "reshape (Reshape)            (None, 30000)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               3840128   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 250)               32250     \n",
      "=================================================================\n",
      "Total params: 3,872,378\n",
      "Trainable params: 3,872,378\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": " Blas GEMM launch failed : a.shape=(128, 30000), b.shape=(30000, 128), m=128, n=128, k=30000\n\t [[node sequential/dense/MatMul (defined at <ipython-input-10-e4c7e2bd9844>:24) ]] [Op:__inference_train_function_593]\n\nFunction call stack:\ntrain_function\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mInternalError\u001B[0m                             Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-10-e4c7e2bd9844>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     22\u001B[0m                   \u001B[0mvalidation_data\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mvalid_data\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     23\u001B[0m                   \u001B[0mepochs\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mnum_epochs\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 24\u001B[0;31m                   validation_steps=valid_steps_per_epoch)\n\u001B[0m\u001B[1;32m     25\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     26\u001B[0m \u001B[0mexit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/tf_gpu/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001B[0m in \u001B[0;36mfit\u001B[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001B[0m\n\u001B[1;32m   1098\u001B[0m                 _r=1):\n\u001B[1;32m   1099\u001B[0m               \u001B[0mcallbacks\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mon_train_batch_begin\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mstep\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1100\u001B[0;31m               \u001B[0mtmp_logs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtrain_function\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0miterator\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1101\u001B[0m               \u001B[0;32mif\u001B[0m \u001B[0mdata_handler\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshould_sync\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1102\u001B[0m                 \u001B[0mcontext\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0masync_wait\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/tf_gpu/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args, **kwds)\u001B[0m\n\u001B[1;32m    826\u001B[0m     \u001B[0mtracing_count\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mexperimental_get_tracing_count\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    827\u001B[0m     \u001B[0;32mwith\u001B[0m \u001B[0mtrace\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mTrace\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_name\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mtm\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 828\u001B[0;31m       \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwds\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    829\u001B[0m       \u001B[0mcompiler\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m\"xla\"\u001B[0m \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_experimental_compile\u001B[0m \u001B[0;32melse\u001B[0m \u001B[0;34m\"nonXla\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    830\u001B[0m       \u001B[0mnew_tracing_count\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mexperimental_get_tracing_count\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/tf_gpu/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001B[0m in \u001B[0;36m_call\u001B[0;34m(self, *args, **kwds)\u001B[0m\n\u001B[1;32m    886\u001B[0m         \u001B[0;31m# Lifting succeeded, so variables are initialized and we can run the\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    887\u001B[0m         \u001B[0;31m# stateless function.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 888\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_stateless_fn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwds\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    889\u001B[0m     \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    890\u001B[0m       \u001B[0m_\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0m_\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0m_\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfiltered_flat_args\u001B[0m \u001B[0;34m=\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m\\\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/tf_gpu/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   2941\u001B[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001B[1;32m   2942\u001B[0m     return graph_function._call_flat(\n\u001B[0;32m-> 2943\u001B[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001B[0m\u001B[1;32m   2944\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2945\u001B[0m   \u001B[0;34m@\u001B[0m\u001B[0mproperty\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/tf_gpu/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001B[0m in \u001B[0;36m_call_flat\u001B[0;34m(self, args, captured_inputs, cancellation_manager)\u001B[0m\n\u001B[1;32m   1917\u001B[0m       \u001B[0;31m# No tape is watching; skip to running the function.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1918\u001B[0m       return self._build_call_outputs(self._inference_function.call(\n\u001B[0;32m-> 1919\u001B[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001B[0m\u001B[1;32m   1920\u001B[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001B[1;32m   1921\u001B[0m         \u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/tf_gpu/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001B[0m in \u001B[0;36mcall\u001B[0;34m(self, ctx, args, cancellation_manager)\u001B[0m\n\u001B[1;32m    558\u001B[0m               \u001B[0minputs\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    559\u001B[0m               \u001B[0mattrs\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mattrs\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 560\u001B[0;31m               ctx=ctx)\n\u001B[0m\u001B[1;32m    561\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    562\u001B[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001B[0;32m~/anaconda3/envs/tf_gpu/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001B[0m in \u001B[0;36mquick_execute\u001B[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001B[0m\n\u001B[1;32m     58\u001B[0m     \u001B[0mctx\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mensure_initialized\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     59\u001B[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001B[0;32m---> 60\u001B[0;31m                                         inputs, attrs, num_outputs)\n\u001B[0m\u001B[1;32m     61\u001B[0m   \u001B[0;32mexcept\u001B[0m \u001B[0mcore\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_NotOkStatusException\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     62\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mname\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mInternalError\u001B[0m:  Blas GEMM launch failed : a.shape=(128, 30000), b.shape=(30000, 128), m=128, n=128, k=30000\n\t [[node sequential/dense/MatMul (defined at <ipython-input-10-e4c7e2bd9844>:24) ]] [Op:__inference_train_function_593]\n\nFunction call stack:\ntrain_function\n"
     ]
    }
   ],
   "source": [
    "Q2 = False\n",
    "Q3 = False\n",
    "if Q2 or Q3:\n",
    "    num_epochs = 20\n",
    "\n",
    "    # try different units\n",
    "\n",
    "    num_labels = train_data.num_classes\n",
    "\n",
    "    hidden_units = 1024\n",
    "    model = Sequential()\n",
    "    model.add(Reshape((-1,), input_shape=(width, width, 3)))\n",
    "    model.add(Dense(hidden_units, activation='relu'))\n",
    "    model.add(Dense(hidden_units, activation='relu'))\n",
    "    model.add(Dense(hidden_units, activation='relu'))\n",
    "    model.add(Dense(num_labels, activation='softmax'))\n",
    "    model.summary()\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    # see if the model is good.\n",
    "    print(model)\n",
    "\n",
    "if Q2 :\n",
    "    history=model.fit(train_data,\n",
    "                      steps_per_epoch =train_steps_per_epoch,\n",
    "                      validation_data=valid_data,\n",
    "                      epochs=num_epochs,\n",
    "                      validation_steps=valid_steps_per_epoch)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Q3. Use the following two call-backs on your best fully connected model and determine if you are able to\n",
    "improve the results. Clearly explain why or why not.\n",
    "    1. LearningRateScheduler\n",
    "    2. ReduceLROnPlateau\n",
    "\n",
    "### LearningRateScheduler\n",
    "from the accuracy of Q2, we would see the accuracy drops from 0.2128 to 0.1944 within epochs 6 to 8, this drop is\n",
    "probably because learning rate is too big, so try to tune LR to increase the accuracy\n",
    "\n",
    "so we have two directions to find the best learning rate as follows, we take the outcome from Q2 as baseline,\n",
    "and we only try 5 epoch for efficiency\n",
    "\n",
    "#### baseline\n",
    "```\n",
    "Epoch 5/20\n",
    "loss: 3.9027 - accuracy: 0.1498 - val_loss: 3.7048 - val_accuracy: 0.1664\n",
    "```\n",
    "\n",
    "#### find the best consistent learning rate\n",
    "we tried a fixed LR from 1e-1 to 1e-10 to see which one is the best\n",
    "\n",
    "```python\n",
    "    lr = 1e-1\n",
    "```\n",
    "```\n",
    " loss: 5.5387 - accuracy: 0.0057 - val_loss: 5.5664 - val_accuracy: 0.0040\n",
    "```\n",
    "\n",
    "```python\n",
    "    lr = 1e-2\n",
    "```\n",
    "```\n",
    "  loss: 4.8003 - accuracy: 0.0375 - val_loss: 4.6560 - val_accuracy: 0.0424\n",
    "```\n",
    "\n",
    "```python\n",
    "    lr = 1e-3\n",
    "```\n",
    "```\n",
    " loss: 3.8051 - accuracy: 0.1694 - val_loss: 3.7018 - val_accuracy: 0.1792\n",
    "```\n",
    "\n",
    "```python\n",
    "    lr = 1e-4\n",
    "```\n",
    "```\n",
    " loss: 3.8836 - accuracy: 0.1757 - val_loss: 3.7352 - val_accuracy: 0.1816\n",
    "```\n",
    "\n",
    "```python\n",
    "    lr = 1e-5\n",
    "```\n",
    "```\n",
    " loss: 4.6433 - accuracy: 0.0939 - val_loss: 4.5639 - val_accuracy: 0.0976\n",
    "```\n",
    "\n",
    "##### Conclusion\n",
    "The accuracy goes lower when LR equals 1e-5, so the best LR would be within 1e-4 and 1e-5\n",
    "\n",
    "#### find the best dynamic learning rate\n",
    "since the best LR would be within 1e-4 and 1e-5, there must be a turning point at a specific epoch.\n",
    "we run a 20-epoch experiment to find this turning point and make LR below it use LR=1e-4 and LR=1e-5\n",
    "for the rest.\n",
    "\n",
    "\n",
    "\n",
    "for LR=1e-4\n",
    "```python\n",
    "    num_epochs = 20\n",
    "    lr = 1e-4\n",
    "```\n",
    "```shell\n",
    "Epoch 10/20\n",
    "poch 9/20\n",
    " loss: 3.2539 - accuracy: 0.2798 - val_loss: 3.2876 - val_accuracy: 0.2592\n",
    "Epoch 10/20\n",
    " loss: 3.1296 - accuracy: 0.3019 - val_loss: 3.2010 - val_accuracy: 0.2776\n",
    "Epoch 11/20\n",
    " loss: 2.9819 - accuracy: 0.3284 - val_loss: 3.2834 - val_accuracy: 0.2552\n",
    "Epoch 12/20\n",
    "```\n",
    "we could see the turning point was at Epoch 10.\n",
    "\n",
    "for LR=1e-4\n",
    "the general performance is better than 1e-4, and even we get a new high at 33.52% accuracy\n",
    "\n",
    "```shell\n",
    "Epoch 19/20\n",
    " loss: 2.1844 - accuracy: 0.4862 - val_loss: 2.9645 - val_accuracy: 0.3352\n",
    "```\n",
    "\n",
    "So we could make 1e-4 as the LR for the first 10 epochs and 1e-5 for the rest\n",
    "\n",
    "```python\n",
    "    # starting point\n",
    "    lr = 1e-4\n",
    "    # dynamic\n",
    "    if epoch > 10:\n",
    "        lr = 1e-5\n",
    "```\n",
    "however, the max accuracy is only 0.3064, lower than consistent LR of 1e-4, which is 33.52%\n",
    "```shell\n",
    "Epoch 20/20\n",
    " loss: 2.6335 - accuracy: 0.4074 - val_loss: 2.9782 - val_accuracy: 0.3064\n",
    "```\n",
    "\n",
    "\n",
    "#### Conclusion\n",
    "The possible reason is that for the gradient descending algorithm, we could only find local optimization,\n",
    "not the global one. So, the epoch 10 drops in local optimization point.\n",
    "\n",
    "\n",
    "So, we will drop this dynamic LR optimization and stick to the previous one, which is that the LR is a consistent\n",
    "number 1e-4.\n",
    "\n",
    "\n",
    "### ReduceLROnPlateau\n",
    "Now we could add a reducer, since we don't find an apparent plateau in the 20 epochs, we tried 100 epoch to see if it works.\n",
    "The min LR is 1e-6, 1% of the basic LR 1e-4 is reasonable from our aspect.\n",
    "\n",
    "\n",
    "```python\n",
    "num_epochs = 10*10\n",
    "\n",
    "lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1),\n",
    "                               cooldown=0,\n",
    "                               patience=5,\n",
    "                               min_lr=1e-6)\n",
    "```\n",
    "\n",
    "Since the patience is 5, which means the learning rate will be reduced once there is no improvement in 5 epochs.\n",
    "It works as expected, take a look at the first example.\n",
    "\n",
    "Epoch 22 is a turning point, so we would expect the accuracy of epoch 24 is highly likely to be lower than that of\n",
    "epoch 23, however, after ajudsting the learning rate, accuracy of Epoch 24 goes up as a new top, which is 0.3544\n",
    "\n",
    "```shell\n",
    "Epoch 21/100\n",
    " loss: 1.9319 - accuracy: 0.5408 - val_loss: 2.9627 - val_accuracy: 0.3440\n",
    "Epoch 22/100\n",
    " loss: 1.8510 - accuracy: 0.5583 - val_loss: 2.9558 - val_accuracy: 0.3488\n",
    "Epoch 23/100\n",
    " loss: 1.7744 - accuracy: 0.5757 - val_loss: 3.0351 - val_accuracy: 0.3296\n",
    "\n",
    "Epoch 00023: ReduceLROnPlateau reducing learning rate to 3.1622775802825264e-05.\n",
    "Epoch 24/100\n",
    " loss: 1.6634 - accuracy: 0.6005 - val_loss: 2.9863 - val_accuracy: 0.3544\n",
    "```\n",
    "\n",
    "This optimization did a great job, we break a new record at epoch 86, which is **0.3928**\n",
    "```\n",
    "Epoch 86/100\n",
    "loss: 0.0059 - accuracy: 0.9999 - val_loss: 5.4967 - val_accuracy: 0.3928\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if Q3:\n",
    "    from tensorflow.keras.callbacks import LearningRateScheduler, ReduceLROnPlateau\n",
    "\n",
    "\n",
    "    # static\n",
    "    def lr_schedule(epoch):\n",
    "        lr = 1e-4\n",
    "\n",
    "        # dynamic optimization, abandoned due to bad performance\n",
    "        # if epoch > 10:\n",
    "        #     lr = 1e-5\n",
    "\n",
    "        return lr\n",
    "\n",
    "    lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "\n",
    "    lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1),\n",
    "                                   cooldown=0,\n",
    "                                   patience=5,\n",
    "                                   min_lr=1e-6)\n",
    "\n",
    "    callbacks = [lr_reducer, lr_scheduler]\n",
    "    history=model.fit(train_data,\n",
    "                  steps_per_epoch =train_steps_per_epoch,\n",
    "                  validation_data=valid_data,\n",
    "                  epochs=num_epochs,\n",
    "                  validation_steps=valid_steps_per_epoch, callbacks=callbacks)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Q4. Use the Keras Hypertune and Random optimizers (https://keras-team.github.io/keras-tuner/) to determine\n",
    "if you can improve the model by varying the number of layers, neurons in each layer and the learning rate.\n",
    "\n",
    "1. Plot the precision vs. recall of the best 20 models in one figure.\n",
    "2. Show a complete evaluation of the top two models.\n",
    "\n",
    "\n",
    "first, we need to install keras-tuner\n",
    "```shell\n",
    "$ conda install -c conda-forge keras-tuner\n",
    "\n",
    "```\n",
    "\n",
    "### Random Search\n",
    "1. Since the best units number we have found manually is 1024, within 512 and 2048, so we fill the two\n",
    "with min and max value\n",
    "2. Since the best learning rate we've found manually is , within 1e-5 and 1e-5, so we will try\n",
    "2e-5, 4e-5, 8e-5\n",
    "3. To improve efficiency, we just try 5 epochs for each combination\n",
    "\n",
    "### Hyper Band\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Q4 = True\n",
    "num_labels = train_data.num_classes\n",
    "tries = 20\n",
    "from kerastuner.tuners import RandomSearch\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "    model.add(Reshape((-1,), input_shape=(width, width, 3)))\n",
    "    model.add(Dense(units=hp.Int('units',\n",
    "                                        min_value=512,\n",
    "                                        max_value=2048,\n",
    "                                        step=128),\n",
    "                           activation='relu'))\n",
    "    model.add(Dense(units=hp.Int('units',\n",
    "                                        min_value=512,\n",
    "                                        max_value=2048,\n",
    "                                        step=128),\n",
    "                           activation='relu'))\n",
    "    model.add(Dense(units=hp.Int('units',\n",
    "                                        min_value=512,\n",
    "                                        max_value=2048,\n",
    "                                        step=128),\n",
    "                           activation='relu'))\n",
    "    model.add(Dense(num_labels, activation='softmax'))\n",
    "    model.compile(\n",
    "        optimizer=Adam(\n",
    "            hp.Choice('learning_rate',\n",
    "                      values=[2e-5, 4e-5, 8e-5])),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def random_search():\n",
    "    tuner = RandomSearch(\n",
    "        build_model,\n",
    "        objective='val_accuracy',\n",
    "        max_trials=tries,\n",
    "        executions_per_trial=1,\n",
    "        directory='my_dir',\n",
    "        project_name='helloworld')\n",
    "\n",
    "    tuner.search_space_summary()\n",
    "\n",
    "    num_epochs = 5\n",
    "    tuner.search(train_data,\n",
    "                  steps_per_epoch =train_steps_per_epoch,\n",
    "                  validation_data=valid_data,\n",
    "                  epochs=num_epochs,\n",
    "                  validation_steps=valid_steps_per_epoch)\n",
    "    tuner.results_summary()\n",
    "    \n",
    "    return tuner.get_best_models(tries)\n",
    "\n",
    "random_models =  random_search()\n",
    "\n",
    "def hyper_band():\n",
    "    from kerastuner.applications import HyperResNet\n",
    "    from kerastuner.tuners import Hyperband\n",
    "\n",
    "    hypermodel = HyperResNet(input_shape=(width, width, 3), num_classes=)\n",
    "\n",
    "    tuner = Hyperband(\n",
    "        hypermodel,\n",
    "        objective='val_accuracy',\n",
    "        max_trials=tries,\n",
    "        directory='my_dir',\n",
    "        project_name='helloworld')\n",
    "\n",
    "    tuner.search(train_data,\n",
    "                  steps_per_epoch =train_steps_per_epoch,\n",
    "                  validation_data=valid_data,\n",
    "                  epochs=num_epochs,\n",
    "                  validation_steps=valid_steps_per_epoch)\n",
    "    \n",
    "    tuner.results_summary()\n",
    "    return tuner.get_best_models(tries)\n",
    "\n",
    "hyper_models = hyper_band()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "from keras import metrics\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer='adam', \n",
    "              metrics=['accuracy',\n",
    "                        metrics.AUC(name='my_auc'),\n",
    "                        F1_Score])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://keras.io/api/callbacks/\n",
    "# We can use a variety of pre-defined callbacks.\n",
    "# Experiment with ReduceLROnPlateuau()\n",
    "\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, CSVLogger, LearningRateScheduler\n",
    "\n",
    "# We can also do a modelcheck point \n",
    "# https://machinelearningmastery.com/check-point-deep-learning-models-keras/\n",
    "  \n",
    "# checkpoint to save the model with best validation accuracy\n",
    "checkpoint = ModelCheckpoint(filepath='model.{epoch:02d}-{val_loss:.2f}.h5', \n",
    "                             monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "# We can also stop the model early\n",
    "#https://machinelearningmastery.com/how-to-stop-training-deep-neural-networks-at-the-right-time-using-early-stopping/\n",
    "# val_loss\n",
    "early_stopping_callback = EarlyStopping(monitor='accuracy', mode='min', verbose=1, patience=200)\n",
    "\n",
    "\n",
    "# initialize TimeStopping callback\n",
    "# https://www.tensorflow.org/addons/tutorials/time_stopping\n",
    "# note that it will still run a minimum of 1 epoch\n",
    "time_stopping_callback = tfa.callbacks.TimeStopping(seconds=600, verbose=1)\n",
    "\n",
    "# We can also use CVSLogger to log information in a CSV\n",
    "csvlogger = CSVLogger(\"logfile.csv\",separator=',',append=False)\n",
    "\n",
    "\n",
    "# ** IMPORTANT ** - please make sure that csvlogger is the last call back\n",
    "# in the list.\n",
    "\n",
    "my_callbacks = [time_stopping_callback,early_stopping_callback,checkpoint,csvlogger]\n",
    "\n",
    "                                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the model with call-backs\n",
    "\n",
    "num_epochs = 1\n",
    "\n",
    "history=model.fit(train_data,\n",
    "                  steps_per_epoch =train_steps_per_epoch, \n",
    "                  validation_data=valid_data,\n",
    "                  epochs=num_epochs,\n",
    "                  validation_steps=valid_steps_per_epoch,\n",
    "                  callbacks=my_callbacks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "from keras import metrics\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer='adam', \n",
    "              metrics=['accuracy',\n",
    "                        metrics.AUC(name='auc'),\n",
    "                        metrics.Precision(name='precision'),\n",
    "                        metrics.Recall(name='recall')])\n",
    "\n",
    "# Fitting the model with more metrics\n",
    "\n",
    "num_epochs = 1\n",
    "\n",
    "history=model.fit(train_data,\n",
    "                  steps_per_epoch =train_steps_per_epoch, \n",
    "                  validation_data=valid_data,\n",
    "                  epochs=num_epochs,\n",
    "                  validation_steps=valid_steps_per_epoch,\n",
    "                  callbacks=my_callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining custom metrics to record while running\n",
    "from keras import backend as K\n",
    "\n",
    "def F1_Score(y_true, y_pred): #taken from old keras source code\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n",
    "    return f1_val\n",
    "\n",
    "def my_metric_fn(y_true, y_pred):\n",
    "    squared_difference = tf.square(y_true - y_pred)\n",
    "    return tf.reduce_mean(squared_difference, axis=-1)  # Note the `axis=-1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "from keras import metrics\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer='adam', \n",
    "              metrics=['accuracy',\n",
    "                        metrics.AUC(name='auc'),\n",
    "                        metrics.Precision(name='precision'),\n",
    "                        metrics.Recall(name='recall'),\n",
    "                        F1_Score])\n",
    "\n",
    "# Fitting the model with more metrics\n",
    "\n",
    "num_epochs = 1\n",
    "\n",
    "history=model.fit(train_data,\n",
    "                  steps_per_epoch =train_steps_per_epoch, \n",
    "                  validation_data=valid_data,\n",
    "                  epochs=num_epochs,\n",
    "                  validation_steps=valid_steps_per_epoch,\n",
    "                  callbacks=my_callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining custom call backs\n",
    "\n",
    "# https://www.tensorflow.org/guide/keras/custom_callback\n",
    "# https://keras.io/guides/writing_your_own_callbacks/\n",
    "\n",
    "from keras.callbacks import Callback\n",
    "import time\n",
    "\n",
    "class TimingCallback(keras.callbacks.Callback):\n",
    "    def __init__(self):\n",
    "        super(TimingCallback, self).__init__()\n",
    "    def on_batch_begin(self, epoch, logs=None):\n",
    "        self.starttime=time.time()\n",
    "    def on_batch_end(self, epoch, logs=None):\n",
    "        logs['epoch_time'] = (time.time()-self.starttime)\n",
    "        print('\\nepoch_time(sec)=',logs['epoch_time'],'\\n')\n",
    "        \n",
    "# create an instance of the timingcallback\n",
    "timing_call = TimingCallback() \n",
    "\n",
    "# We can also use other metrics\n",
    "# https://keras.io/api/metrics/\n",
    "class PrintBatchCallback(keras.callbacks.Callback):  \n",
    "    def on_train_batch_end(self, batch, logs=None):\n",
    "        print(\"For batch {}, loss is {:7.2f}.\".format(batch, logs[\"loss\"]))\n",
    "        print(\"For batch {}, accuracy is {:7.2f}.\".format(batch, logs[\"accuracy\"]))\n",
    "        print(\"For batch {}, AUC is {:7.2f}.\".format(batch, logs[\"auc\"]))\n",
    "\n",
    "print_batch_call = PrintBatchCallback()\n",
    "\n",
    "# add to the callback list\n",
    "my_callbacks = [time_stopping_callback,early_stopping_callback,checkpoint,print_batch_call, timing_call, CSVLogger('new.csv', separator=',')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "from keras import metrics\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer='adam', \n",
    "              metrics=['accuracy',\n",
    "                        metrics.AUC(name='auc'),\n",
    "                        metrics.Precision(name='precision'),\n",
    "                        metrics.Recall(name='recall'),\n",
    "                        F1_Score])\n",
    "\n",
    "# Fitting the model with more metrics\n",
    "\n",
    "num_epochs = 1\n",
    "\n",
    "history=model.fit(train_data,\n",
    "                  steps_per_epoch =train_steps_per_epoch, \n",
    "                  validation_data=valid_data,\n",
    "                  epochs=num_epochs,\n",
    "                  validation_steps=valid_steps_per_epoch,\n",
    "                  callbacks=my_callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://neptune.ai/blog/keras-metrics\n",
    "\n",
    "# How to save batch level data in a file \n",
    "\n",
    "import os\n",
    "from keras.callbacks import Callback\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class SaveBatchLevelDataCallback(keras.callbacks.Callback):\n",
    "    def __init__(self, validation_data, save_dir):\n",
    "        super().__init__()\n",
    "        self.validation_data = validation_data\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        self.save_dir = save_dir\n",
    "        self.f = None\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        # create a file\n",
    "        self.f= open(os.path.join(self.save_dir, f'epoch_{epoch}.csv'),'w+')\n",
    "        line = \"batch,loss,accuracy,auc\\n\"\n",
    "        self.f.write(line)\n",
    "    \n",
    "    def on_epoch_end(self, batch, logs=None):\n",
    "        self.f.close()\n",
    "        \n",
    "    def on_train_batch_end(self, batch, logs=None):\n",
    "        line = \"{},{:7.2f},{:7.2f},{:7.2f}\\n\".format(batch, logs[\"loss\"], logs[\"accuracy\"],logs[\"auc\"])\n",
    "        self.f.write(line)\n",
    "        \n",
    "    \n",
    "batch_write_cbk = SaveBatchLevelDataCallback(validation_data=valid_data,save_dir='batch_data')\n",
    "\n",
    "# add to the callback list\n",
    "my_callbacks = [time_stopping_callback,early_stopping_callback,checkpoint,batch_write_cbk, CSVLogger('new.csv', separator=',')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Compile the model\n",
    "from keras import metrics\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer='adam', \n",
    "              metrics=['accuracy',\n",
    "                        metrics.AUC(name='auc'),\n",
    "                        metrics.Precision(name='precision'),\n",
    "                        metrics.Recall(name='recall'),\n",
    "                        F1_Score])\n",
    "\n",
    "# Fitting the model with more metrics\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "history=model.fit(train_data,\n",
    "                  steps_per_epoch =train_steps_per_epoch, \n",
    "                  validation_data=valid_data,\n",
    "                  epochs=num_epochs,\n",
    "                  validation_steps=valid_steps_per_epoch,\n",
    "                  callbacks=my_callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print history \n",
    "print(history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot accuracy vs epoch\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validate'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot loss values vs epoch\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validate'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot loss values vs epoch\n",
    "plt.plot(history.history['F1_Score'])\n",
    "plt.plot(history.history['val_F1_Score'])\n",
    "plt.title('Model F1-Score')\n",
    "plt.ylabel('F1_Score')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validate'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot accuracy vs. prevision\n",
    "plt.plot(history.history['precision'],label='precision')\n",
    "plt.plot(history.history['val_precision'],label='val_precision')\n",
    "plt.plot(history.history['recall'],label='recall')\n",
    "plt.plot(history.history['val_recall'],label='val_precision')\n",
    "plt.title('Model Precision and Recall')\n",
    "plt.ylabel('Precision and Recall')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot accuracy vs. prevision\n",
    "plt.plot(history.history['precision'],history.history['recall'],'o', color='black',label='precision vs. recall')\n",
    "plt.plot(history.history['recall'],history.history['val_recall'],'o', color='red',label='val_precision vs. val_recall')\n",
    "plt.title('Model Precision and Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.xlabel('Recall')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Evaluate against test data.\n",
    "scores = model.evaluate(test_data, verbose=1)\n",
    "\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])\n",
    "print('Test AUC:', scores[1])\n",
    "print('Test precision:', scores[1])\n",
    "print('Test recall:', scores[1])\n",
    "print('Test F1-Score:', scores[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For evaluation first, we will create the actual and predicted labels\n",
    "# We can then use these to generate all the reports we need.\n",
    "\n",
    "# make predictions on the testing images, finding the index of the\n",
    "# label with the corresponding largest predicted probability\n",
    "\n",
    "predicted = model.predict(x=test_data, steps=test_steps_per_epoch)\n",
    "\n",
    "# create predited IDs\n",
    "predicted = np.argmax(predicted, axis=1)\n",
    "\n",
    "# create test labels from the generator\n",
    "actual = []\n",
    "for i in range(0,int(test_steps_per_epoch)):\n",
    "    actual.extend(np.array(test_data[i][1]))\n",
    "\n",
    "# create actual IDs\n",
    "actual = np.asarray(actual).argmax(axis=1)\n",
    "\n",
    "# make sure predicted and actual are the same size and shape\n",
    "print(predicted.shape)\n",
    "print(actual.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(\"[INFO] evaluating network...\")\n",
    "print(classification_report(actual, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can determine the confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm=confusion_matrix(actual,predicted)\n",
    "\n",
    "def print_cm(cm, frm, to,abs_or_relative=0):\n",
    "    import seaborn as sns\n",
    "    import matplotlib.pylab as plt\n",
    "\n",
    "    cm = cm[frm:to+1,frm:to+1]\n",
    "    # create labels\n",
    "    x_axis_labels = np.arange(frm,to+1)\n",
    "    y_axis_labels = np.arange(frm,to+1)\n",
    "    \n",
    "    plt.xticks(rotation=45)\n",
    "    plt.yticks(rotation=-45)\n",
    "    \n",
    "    if(abs_or_relative==0):\n",
    "        sns.heatmap(cm, annot=True,xticklabels=x_axis_labels, yticklabels=y_axis_labels)\n",
    "    else:\n",
    "        sns.heatmap(cm/np.sum(cm), annot=True, \n",
    "           fmt='.2%', cmap='Blues',\n",
    "           xticklabels=x_axis_labels, yticklabels=y_axis_labels)\n",
    "\n",
    "print_cm(cm,1 ,20,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we already have actual and predicted \n",
    "\n",
    "# also see https://www.dlology.com/blog/simple-guide-on-how-to-generate-roc-plot-for-keras-classifier/\n",
    "# for micro-average ROC curves as well\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "\n",
    "#extract the actual labels from the test data\n",
    "Y_test = []\n",
    "for i in range(0,int(test_steps_per_epoch)):\n",
    "    Y_test.extend(np.array(test_data[i][1]))\n",
    "Y_test = np.array(Y_test)\n",
    "n_classes = Y_test.shape[1]  # one hot encoded\n",
    "\n",
    "# create actual output from the model using test_data\n",
    "y_score=model.predict(x=test_data, steps=test_steps_per_epoch)\n",
    "\n",
    "print(Y_test.shape)\n",
    "print(y_score.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(n_classes)\n",
    "# compare each class's probabilities one by one\n",
    "# each acts like a single column\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(Y_test[:,i], y_score[:,i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# Print the AUC scores\n",
    "from IPython.display import display\n",
    "import pandas as pd\n",
    "auc_array = np.array(list(roc_auc.items()))\n",
    "df = pd.DataFrame(auc_array[:,1])\n",
    "df.columns = ['AUC']\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the ROC for the ith class cls\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "def plot_roc(cls,roc_dir):  \n",
    "    plt.plot(fpr[cls], tpr[cls], lw=2,label='ROC curve of class {0} (area = {1:0.3f})'\n",
    "    ''.format(cls, roc_auc[cls]))\n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(roc_dir, f'ROC_{cls}.png'))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# make sure directory exists\n",
    "def make_directory(roc_dir):\n",
    "    try:\n",
    "        os.mkdir(roc_dir)\n",
    "    except OSError:\n",
    "        print (\"Creation of the directory %s failed\" % roc_dir)\n",
    "    else:\n",
    "        print (\"Successfully created the directory %s \" % roc_dir)\n",
    "        \n",
    "# print the roc curve for 0\n",
    "\n",
    "make_directory('rocs')\n",
    "\n",
    "for i in range(n_classes):\n",
    "    plot_roc(i,'rocs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using tensorflow extension\n",
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define tensorboard callback\n",
    "\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "# Using remote tensorboard\n",
    "#https://blog.yyliu.net/remote-tensorboard/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "from keras import metrics\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer='adam', \n",
    "              metrics=['accuracy',\n",
    "                        metrics.AUC(name='auc'),\n",
    "                        metrics.Precision(name='precision'),\n",
    "                        metrics.Recall(name='recall')])\n",
    "\n",
    "# Fitting the model with more metrics\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "history=model.fit(train_data,\n",
    "                  steps_per_epoch =train_steps_per_epoch, \n",
    "                  validation_data=valid_data,\n",
    "                  epochs=num_epochs,\n",
    "                  validation_steps=valid_steps_per_epoch,\n",
    "                  callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}