{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "elder-extension",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# Author : Ali Mirzaei\n",
    "# Date : 19/09/2017\n",
    "\n",
    "\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Input, Flatten, Reshape, Conv2DTranspose, concatenate\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.optimizers import Adam,SGD\n",
    "from tensorflow.keras.initializers import RandomNormal\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "from tensorflow.keras.applications import imagenet_utils\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image, display\n",
    "import random\n",
    "import math\n",
    "import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.utils import plot_model \n",
    "from tensorflow.keras.layers import BatchNormalization, Input, Dense, Dropout, Flatten, Activation,Concatenate,LeakyReLU\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, AveragePooling2D, UpSampling2D, AvgPool2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import backend, models\n",
    "#import tensorflow_addons as tfa\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n",
    "# need to add these for the GPU\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.compat.v1.Session(config=config)\n",
    "\n",
    "# import the image generator\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "sweet-program",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-7-0ecbc0c1a484>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     23\u001B[0m \u001B[0;31m# needs to be rescaled for the neural network\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     24\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 25\u001B[0;31m \u001B[0mtrain_data\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mgenerator\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mflow_from_directory\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtrain_dir\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtarget_size\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mwidth\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mwidth\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mbatch_size\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m10000\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     26\u001B[0m \u001B[0mvalid_data\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mgenerator\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mflow_from_directory\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mvalid_dir\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtarget_size\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mwidth\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mwidth\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mbatch_size\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m1000\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     27\u001B[0m \u001B[0mtest_data\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mgenerator\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mflow_from_directory\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtest_dir\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtarget_size\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mwidth\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mwidth\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mbatch_size\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m1000\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/envs/tf/lib/python3.7/site-packages/keras_preprocessing/image/image_data_generator.py\u001B[0m in \u001B[0;36mflow_from_directory\u001B[0;34m(self, directory, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation)\u001B[0m\n\u001B[1;32m    541\u001B[0m             \u001B[0msubset\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0msubset\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    542\u001B[0m             \u001B[0minterpolation\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0minterpolation\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 543\u001B[0;31m             \u001B[0mdtype\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdtype\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    544\u001B[0m         )\n\u001B[1;32m    545\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/envs/tf/lib/python3.7/site-packages/keras_preprocessing/image/directory_iterator.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, directory, image_data_generator, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, data_format, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation, dtype)\u001B[0m\n\u001B[1;32m    133\u001B[0m         \u001B[0mclasses_list\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    134\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mres\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mresults\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 135\u001B[0;31m             \u001B[0mclasses\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfilenames\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mres\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    136\u001B[0m             \u001B[0mclasses_list\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mclasses\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    137\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfilenames\u001B[0m \u001B[0;34m+=\u001B[0m \u001B[0mfilenames\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/envs/tf/lib/python3.7/multiprocessing/pool.py\u001B[0m in \u001B[0;36mget\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    649\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    650\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mget\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtimeout\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mNone\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 651\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwait\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtimeout\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    652\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mready\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    653\u001B[0m             \u001B[0;32mraise\u001B[0m \u001B[0mTimeoutError\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/envs/tf/lib/python3.7/multiprocessing/pool.py\u001B[0m in \u001B[0;36mwait\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    646\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    647\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mwait\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtimeout\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mNone\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 648\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_event\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwait\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtimeout\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    649\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    650\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mget\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtimeout\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mNone\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/envs/tf/lib/python3.7/threading.py\u001B[0m in \u001B[0;36mwait\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    550\u001B[0m             \u001B[0msignaled\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_flag\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    551\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0msignaled\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 552\u001B[0;31m                 \u001B[0msignaled\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_cond\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwait\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtimeout\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    553\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0msignaled\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    554\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/envs/tf/lib/python3.7/threading.py\u001B[0m in \u001B[0;36mwait\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    294\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m    \u001B[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    295\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mtimeout\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 296\u001B[0;31m                 \u001B[0mwaiter\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0macquire\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    297\u001B[0m                 \u001B[0mgotit\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mTrue\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    298\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "#Setting the parameters for training\n",
    "\n",
    "# batch size and image width to use\n",
    "batch_size=128\n",
    "width=100\n",
    "\n",
    "# all the data directories\n",
    "train_dir='train/';\n",
    "test_dir='test/'\n",
    "valid_dir='valid/';\n",
    "\n",
    "# the number of epochs\n",
    "num_epochs=5000\n",
    "\n",
    "# creating an image generator that will feed the data from\n",
    "# each of the directories\n",
    "\n",
    "# we use scaling transformation in this generator\n",
    "generator=ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# we specify the size of the input and batch size\n",
    "# size of the input is necessary because the image\n",
    "# needs to be rescaled for the neural network\n",
    "\n",
    "train_data=generator.flow_from_directory(train_dir, target_size=(width,width),batch_size=10000)\n",
    "valid_data=generator.flow_from_directory(valid_dir, target_size=(width,width),batch_size=1000)\n",
    "test_data=generator.flow_from_directory(test_dir, target_size=(width,width),batch_size=1000)\n",
    "\n",
    "# the number of steps per epoch is samples/batch size\n",
    "# we need to use these numbers later\n",
    "\n",
    "train_steps_per_epoch=math.ceil(train_data.samples/batch_size)\n",
    "valid_steps_per_epoch=math.ceil(valid_data.samples/batch_size)\n",
    "test_steps_per_epoch=math.ceil(test_data.samples/batch_size)\n",
    "\n",
    "def rgb2gray(rgb):\n",
    "    \"\"\"Convert from color image (RGB) to grayscale.\n",
    "       Source: opencv.org\n",
    "       grayscale = 0.299*red + 0.587*green + 0.114*blue\n",
    "    Argument:\n",
    "        rgb (tensor): rgb image\n",
    "    Return:\n",
    "        (tensor): grayscale image\n",
    "    \"\"\"\n",
    "    return np.dot(rgb[...,:3], [0.299, 0.587, 0.114])\n",
    "\n",
    "x_train, y_train = train_data.next()\n",
    "x_test, y_test = test_data.next()\n",
    "x_train.shape\n",
    "num_labels = len(train_data.class_indices)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "def show_image(x):\n",
    "    plt.imshow(np.clip(x, 0, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "painted-annual",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image(x_train[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "difficult-third",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southern-republic",
   "metadata": {},
   "outputs": [],
   "source": [
    "initializer = RandomNormal(mean=0.0, stddev=0.01, seed=None)\n",
    "from keras import backend as K\n",
    "\n",
    "epochs=5000\n",
    "\n",
    "img_rows = width\n",
    "img_cols = width\n",
    "channels = 3\n",
    "\n",
    "# network parameters\n",
    "input_shape = (img_rows, img_cols, channels)\n",
    "batch_size = 128\n",
    "kernel_size = 3\n",
    "filters = 16\n",
    "latent_dim = 256\n",
    "# encoder/decoder number of CNN layers and filters per layer\n",
    "layer_filters = [32]\n",
    "\n",
    "class SAAE():\n",
    "    def __init__(self, img_shape=input_shape, encoded_dim=latent_dim):\n",
    "        self.encoded_dim = encoded_dim\n",
    "        self.optimizer_reconst = Adam(0.001)\n",
    "        self.optimizer_discriminator = Adam(0.0001)\n",
    "        self._initAndCompileFullModel(img_shape, encoded_dim)\n",
    "\n",
    "    def _genEncoderModel(self, img_shape, encoded_dim):\n",
    "        \"\"\" Build Encoder Model Based on Paper Configuration\n",
    "        Args:\n",
    "            img_shape (tuple) : shape of input image\n",
    "            encoded_dim (int) : number of latent variables\n",
    "        Return:\n",
    "            A sequential keras model\n",
    "        \"\"\"\n",
    "        input_shape = img_shape\n",
    "        latent_dim = encoded_dim\n",
    "\n",
    "        inputs = Input(shape=input_shape, name='encoder_input')\n",
    "        x = inputs\n",
    "        # stack of Conv2D(64)-Conv2D(128)-Conv2D(256)\n",
    "        for filters in layer_filters:\n",
    "            x = Conv2D(filters=filters,\n",
    "                       kernel_size=kernel_size,\n",
    "                       strides=2,\n",
    "                       activation='relu',\n",
    "                       padding='same')(x)\n",
    "\n",
    "        # shape info needed to build decoder model so we don't do hand computation\n",
    "        # the input to the decoder's first Conv2DTranspose will have this shape\n",
    "        # shape is (4, 4, 256) which is processed by the decoder back to (32, 32, 3)\n",
    "        self.shape = K.int_shape(x)\n",
    "\n",
    "        # generate a latent vector\n",
    "        x = Flatten()(x)\n",
    "        latent = Dense(latent_dim, name='latent_vector')(x)\n",
    "\n",
    "        # instantiate encoder model\n",
    "        encoder = Model(inputs, latent, name='encoder')\n",
    "        encoder.summary()\n",
    "        return encoder\n",
    "\n",
    "    def _getDecoderModel(self, encoded_dim, img_shape):\n",
    "        \"\"\" Build Decoder Model Based on Paper Configuration\n",
    "        Args:\n",
    "            encoded_dim (int) : number of latent variables\n",
    "            img_shape (tuple) : shape of target images\n",
    "        Return:\n",
    "            A sequential keras model\n",
    "        \"\"\"\n",
    "        latent_dim = encoded_dim\n",
    "        shape = self.shape\n",
    "\n",
    "        # build the decoder model\n",
    "        latent_inputs = Input(shape=(latent_dim,), name='decoder_input')\n",
    "        labels_inputs = Input(shape=(num_labels,))\n",
    "        concated = concatenate([latent_inputs, labels_inputs])\n",
    "        x = Dense(shape[1] * shape[2] * shape[3])(concated)\n",
    "        x = Reshape((shape[1], shape[2], shape[3]))(x)\n",
    "\n",
    "        # stack of Conv2DTranspose(256)-Conv2DTranspose(128)-Conv2DTranspose(64)\n",
    "        for filters in layer_filters[::-1]:\n",
    "            x = Conv2DTranspose(filters=filters,\n",
    "                                kernel_size=kernel_size,\n",
    "                                strides=2,\n",
    "                                activation='relu',\n",
    "                                padding='same')(x)\n",
    "\n",
    "        outputs = Conv2DTranspose(filters=channels,\n",
    "                                  kernel_size=kernel_size,\n",
    "                                  activation='sigmoid',\n",
    "                                  padding='same',\n",
    "                                  name='decoder_output')(x)\n",
    "\n",
    "        # instantiate decoder model\n",
    "        decoder = Model([latent_inputs, labels_inputs], outputs, name='decoder')\n",
    "        decoder.summary()\n",
    "\n",
    "        return decoder\n",
    "\n",
    "    def _getDescriminator(self, encoded_dim):\n",
    "        \"\"\" Build Descriminator Model Based on Paper Configuration\n",
    "        Args:\n",
    "            encoded_dim (int) : number of latent variables\n",
    "        Return:\n",
    "            A sequential keras model\n",
    "        \"\"\"\n",
    "        latent_dim = encoded_dim\n",
    "        shape = self.shape\n",
    "\n",
    "        # build the decoder model\n",
    "        latent_inputs = Input(shape=(latent_dim,), name='discriminator_input')\n",
    "        x = Dense(shape[1]*shape[2]*shape[3], kernel_initializer=initializer,\n",
    "                          bias_initializer=initializer)(latent_inputs)\n",
    "        x = Reshape((shape[1], shape[2], shape[3]))(x)\n",
    "\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation('relu')(x)\n",
    "        x = Conv2D(filters=32, kernel_size=3, activation='relu', padding='same')(x)\n",
    "        x = AvgPool2D(pool_size=2, strides=2)(x)\n",
    "        x = Conv2D(filters=16, kernel_size=3, activation='relu')(x)\n",
    "        x = AvgPool2D(pool_size=2, strides=2)(x)\n",
    "        x = Flatten()(x)\n",
    "\n",
    "        outputs = Dense(1, name='discriminator_output', activation='sigmoid',\n",
    "                        kernel_initializer=initializer, bias_initializer=initializer)(x)\n",
    "\n",
    "        \n",
    "        # instantiate discriminator model\n",
    "        discriminator = Model(latent_inputs, outputs, name='discriminator')\n",
    "        discriminator.summary()\n",
    "\n",
    "        return discriminator\n",
    "\n",
    "    def _initAndCompileFullModel(self, img_shape, encoded_dim):\n",
    "        self.encoder = self._genEncoderModel(img_shape, encoded_dim)\n",
    "        self.decoder = self._getDecoderModel(encoded_dim, img_shape)\n",
    "        self.discriminator = self._getDescriminator(encoded_dim)\n",
    "        img = Input(shape=img_shape)\n",
    "        label = Input(shape=(num_labels,))\n",
    "        encoded_repr = self.encoder(img)\n",
    "        gen_img = self.decoder([encoded_repr, label])\n",
    "        self.autoencoder = Model([img, label], gen_img)\n",
    "        valid = self.discriminator(encoded_repr)\n",
    "        self.encoder_discriminator = Model(img, valid)\n",
    "        self.discriminator.compile(optimizer=self.optimizer_discriminator,\n",
    "                                   loss='binary_crossentropy',\n",
    "                                   metrics=['accuracy'])\n",
    "        self.autoencoder.compile(optimizer=self.optimizer_reconst,\n",
    "                                 loss ='mse')\n",
    "        for layer in self.discriminator.layers:\n",
    "            layer.trainable = False\n",
    "        self.encoder_discriminator.compile(optimizer=self.optimizer_discriminator,\n",
    "                                           loss='binary_crossentropy',\n",
    "                                           metrics=['accuracy'])\n",
    "    def imagegrid(self, epochnumber):\n",
    "        fig = plt.figure(figsize=[20, 20])\n",
    "        labels = y_train\n",
    "        styles = np.random.normal(size=(10, self.encoded_dim))\n",
    "        k=0\n",
    "        for label in labels[epochnumber//num_labels:epochnumber//num_labels+10]:\n",
    "            for style in styles:\n",
    "                img = self.decoder.predict([style.reshape(-1,self.encoded_dim), label.reshape(-1,num_labels)])\n",
    "                img = img.reshape(input_shape)\n",
    "                k = k + 1\n",
    "                # 250 rows, 10 colums\n",
    "                ax = fig.add_subplot(10, 10, k)\n",
    "                ax.set_axis_off()\n",
    "                ax.imshow(img, cmap=\"gray\")\n",
    "        fig.savefig(\"images/SAAE/\" + str(epochnumber) + \".png\")\n",
    "        plt.show()\n",
    "        plt.close(fig)\n",
    "\n",
    "\n",
    "    def train(self, x_train, y_train, batch_size=32, epochs=5000, save_image_interval=100):\n",
    "        labels = y_train\n",
    "        for epoch in range(epochs):\n",
    "            #---------------Train Discriminator -------------\n",
    "            # Select a random half batch of images\n",
    "            idx = np.random.randint(0, x_train.shape[0], batch_size)\n",
    "            imgs = x_train[idx]\n",
    "            labels_batch = labels[idx, :]\n",
    "            # Generate a half batch of new images\n",
    "            latent_fake = self.encoder.predict(imgs)\n",
    "            #gen_imgs = self.decoder.predict(latent_fake)\n",
    "            latent_real = np.random.normal(size=(batch_size, self.encoded_dim))\n",
    "            valid = np.ones((batch_size, 1))\n",
    "            fake = np.zeros((batch_size, 1))\n",
    "            # Train the discriminator\n",
    "            d_loss_real = self.discriminator.train_on_batch(latent_real, valid)\n",
    "            d_loss_fake = self.discriminator.train_on_batch(latent_fake, fake)\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "            #idx = np.random.randint(0, x_train.shape[0], batch_size)\n",
    "            #imgs = x_train[idx]\n",
    "            # Generator wants the discriminator to label the generated representations as valid\n",
    "            valid_y = np.ones((batch_size, 1))\n",
    "\n",
    "            # Train the autoencode reconstruction\n",
    "            g_loss_reconstruction = self.autoencoder.train_on_batch([imgs, labels_batch], imgs)\n",
    "\n",
    "            # Train generator\n",
    "            g_logg_similarity = self.encoder_discriminator.train_on_batch(imgs, valid_y)\n",
    "            # Plot the progress\n",
    "            if(epoch % save_image_interval == 0):\n",
    "                print (\"%d [D loss: %f, acc: %.2f%%] [G acc: %f, mse: %f]\" % (epoch, d_loss[0], 100*d_loss[1],\n",
    "                   g_logg_similarity[1], g_loss_reconstruction))\n",
    "                self.imagegrid(epoch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reasonable-disease",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "ann = SAAE()\n",
    "ann.train(x_train, y_train, batch_size, epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legal-crazy",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}