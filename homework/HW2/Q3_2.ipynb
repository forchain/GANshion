{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elder-extension",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# Author : Ali Mirzaei\n",
    "# Date : 19/09/2017\n",
    "\n",
    "\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Input, Flatten, Reshape, Conv2DTranspose, concatenate\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.optimizers import Adam,SGD\n",
    "from tensorflow.keras.initializers import RandomNormal\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "from tensorflow.keras.applications import imagenet_utils\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image, display\n",
    "import random\n",
    "import math\n",
    "import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.utils import plot_model \n",
    "from tensorflow.keras.layers import BatchNormalization, Input, Dense, Dropout, Flatten, Activation,Concatenate,LeakyReLU\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, AveragePooling2D, UpSampling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import backend, models\n",
    "#import tensorflow_addons as tfa\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n",
    "# need to add these for the GPU\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.compat.v1.Session(config=config)\n",
    "\n",
    "# import the image generator\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sweet-program",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting the parameters for training\n",
    "\n",
    "# batch size and image width to use\n",
    "batch_size=128\n",
    "width=100\n",
    "\n",
    "# all the data directories\n",
    "train_dir='train/';\n",
    "test_dir='test/'\n",
    "valid_dir='valid/';\n",
    "\n",
    "# the number of epochs\n",
    "num_epochs=5000\n",
    "\n",
    "# creating an image generator that will feed the data from\n",
    "# each of the directories\n",
    "\n",
    "# we use scaling transformation in this generator\n",
    "generator=ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# we specify the size of the input and batch size\n",
    "# size of the input is necessary because the image\n",
    "# needs to be rescaled for the neural network\n",
    "\n",
    "train_data=generator.flow_from_directory(train_dir, target_size=(width,width),batch_size=10000)\n",
    "valid_data=generator.flow_from_directory(valid_dir, target_size=(width,width),batch_size=1000)\n",
    "test_data=generator.flow_from_directory(test_dir, target_size=(width,width),batch_size=1000)\n",
    "\n",
    "# the number of steps per epoch is samples/batch size\n",
    "# we need to use these numbers later\n",
    "\n",
    "train_steps_per_epoch=math.ceil(train_data.samples/batch_size)\n",
    "valid_steps_per_epoch=math.ceil(valid_data.samples/batch_size)\n",
    "test_steps_per_epoch=math.ceil(test_data.samples/batch_size)\n",
    "\n",
    "def rgb2gray(rgb):\n",
    "    \"\"\"Convert from color image (RGB) to grayscale.\n",
    "       Source: opencv.org\n",
    "       grayscale = 0.299*red + 0.587*green + 0.114*blue\n",
    "    Argument:\n",
    "        rgb (tensor): rgb image\n",
    "    Return:\n",
    "        (tensor): grayscale image\n",
    "    \"\"\"\n",
    "    return np.dot(rgb[...,:3], [0.299, 0.587, 0.114])\n",
    "\n",
    "x_train, y_train = train_data.next()\n",
    "x_test, y_test = test_data.next()\n",
    "x_train.shape\n",
    "num_labels = len(train_data.class_indices)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "def show_image(x):\n",
    "    plt.imshow(np.clip(x, 0, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "painted-annual",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image(x_train[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "difficult-third",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southern-republic",
   "metadata": {},
   "outputs": [],
   "source": [
    "initializer = RandomNormal(mean=0.0, stddev=0.01, seed=None)\n",
    "from keras import backend as K\n",
    "\n",
    "epochs=5000\n",
    "\n",
    "img_rows = width\n",
    "img_cols = width\n",
    "channels = 3\n",
    "\n",
    "# network parameters\n",
    "input_shape = (img_rows, img_cols, channels)\n",
    "batch_size = 128\n",
    "kernel_size = 3\n",
    "filters = 16\n",
    "latent_dim = 256\n",
    "# encoder/decoder number of CNN layers and filters per layer\n",
    "layer_filters = [32]\n",
    "\n",
    "class SAAE():\n",
    "    def __init__(self, img_shape=input_shape, encoded_dim=latent_dim):\n",
    "        self.encoded_dim = encoded_dim\n",
    "        self.optimizer_reconst = Adam(0.001)\n",
    "        self.optimizer_discriminator = Adam(0.0001)\n",
    "        self._initAndCompileFullModel(img_shape, encoded_dim)\n",
    "\n",
    "    def _genEncoderModel(self, img_shape, encoded_dim):\n",
    "        \"\"\" Build Encoder Model Based on Paper Configuration\n",
    "        Args:\n",
    "            img_shape (tuple) : shape of input image\n",
    "            encoded_dim (int) : number of latent variables\n",
    "        Return:\n",
    "            A sequential keras model\n",
    "        \"\"\"\n",
    "        input_shape = img_shape\n",
    "        latent_dim = encoded_dim\n",
    "\n",
    "        inputs = Input(shape=input_shape, name='encoder_input')\n",
    "        x = inputs\n",
    "        # stack of Conv2D(64)-Conv2D(128)-Conv2D(256)\n",
    "        for filters in layer_filters:\n",
    "            x = Conv2D(filters=filters,\n",
    "                       kernel_size=kernel_size,\n",
    "                       strides=2,\n",
    "                       activation='relu',\n",
    "                       padding='same')(x)\n",
    "\n",
    "        # shape info needed to build decoder model so we don't do hand computation\n",
    "        # the input to the decoder's first Conv2DTranspose will have this shape\n",
    "        # shape is (4, 4, 256) which is processed by the decoder back to (32, 32, 3)\n",
    "        self.shape = K.int_shape(x)\n",
    "\n",
    "        # generate a latent vector\n",
    "        x = Flatten()(x)\n",
    "        latent = Dense(latent_dim, name='latent_vector')(x)\n",
    "\n",
    "        # instantiate encoder model\n",
    "        encoder = Model(inputs, latent, name='encoder')\n",
    "        encoder.summary()\n",
    "        return encoder\n",
    "\n",
    "    def _getDecoderModel(self, encoded_dim, img_shape):\n",
    "        \"\"\" Build Decoder Model Based on Paper Configuration\n",
    "        Args:\n",
    "            encoded_dim (int) : number of latent variables\n",
    "            img_shape (tuple) : shape of target images\n",
    "        Return:\n",
    "            A sequential keras model\n",
    "        \"\"\"\n",
    "        latent_dim = encoded_dim\n",
    "        shape = self.shape\n",
    "\n",
    "        # build the decoder model\n",
    "        latent_inputs = Input(shape=(latent_dim,), name='decoder_input')\n",
    "        x = Dense(shape[1]*shape[2]*shape[3])(latent_inputs)\n",
    "        x = Reshape((shape[1], shape[2], shape[3]))(x)\n",
    "\n",
    "        # stack of Conv2DTranspose(256)-Conv2DTranspose(128)-Conv2DTranspose(64)\n",
    "        for filters in layer_filters[::-1]:\n",
    "            x = Conv2DTranspose(filters=filters,\n",
    "                                kernel_size=kernel_size,\n",
    "                                strides=2,\n",
    "                                activation='relu',\n",
    "                                padding='same')(x)\n",
    "\n",
    "        outputs = Conv2DTranspose(filters=channels,\n",
    "                                  kernel_size=kernel_size,\n",
    "                                  activation='sigmoid',\n",
    "                                  padding='same',\n",
    "                                  name='decoder_output')(x)\n",
    "\n",
    "        # instantiate decoder model\n",
    "        decoder = Model(latent_inputs, outputs, name='decoder')\n",
    "        decoder.summary()\n",
    "\n",
    "        return decoder\n",
    "\n",
    "    def _getDescriminator(self, encoded_dim):\n",
    "        \"\"\" Build Descriminator Model Based on Paper Configuration\n",
    "        Args:\n",
    "            encoded_dim (int) : number of latent variables\n",
    "        Return:\n",
    "            A sequential keras model\n",
    "        \"\"\"\n",
    "        input_shape = encoded_dim\n",
    "        shape = self.shape\n",
    "\n",
    "        # build the decoder model\n",
    "        latent_inputs = Input(shape=(latent_dim,), name='discriminator_input')\n",
    "        x = Dense(shape[1]*shape[2]*shape[3], kernel_initializer=initializer,\n",
    "                          bias_initializer=initializer)(latent_inputs)\n",
    "        x = Reshape((shape[1], shape[2], shape[3]))(x)\n",
    "\n",
    "        # Stack of BN-ReLU-Transposed Conv2D-UpSampling blocks\n",
    "        for filters in layer_filters[::-1]:\n",
    "            #x = BatchNormalization()(x)\n",
    "            x = Activation('relu')(x)\n",
    "            x = Conv2DTranspose(filters=filters,\n",
    "                                kernel_size=kernel_size,\n",
    "                                padding='same', kernel_initializer=initializer,\n",
    "                          bias_initializer=initializer)(x)\n",
    "            x = UpSampling2D()(x)\n",
    "\n",
    "        x = Flatten()(x)\n",
    "        outputs = Dense(1, name='discriminator_output', activation='sigmoid',\n",
    "                        kernel_initializer=initializer, bias_initializer=initializer)(x)\n",
    "\n",
    "        # instantiate discriminator model\n",
    "        discriminator = Model(latent_inputs, outputs, name='discriminator')\n",
    "        discriminator.summary()\n",
    "\n",
    "\n",
    "        return discriminator\n",
    "\n",
    "    def _initAndCompileFullModel(self, img_shape, encoded_dim):\n",
    "        self.encoder = self._genEncoderModel(img_shape, encoded_dim)\n",
    "        self.decoder = self._getDecoderModel(encoded_dim, img_shape)\n",
    "        self.discriminator = self._getDescriminator(encoded_dim)\n",
    "        img = Input(shape=img_shape)\n",
    "        label = Input(shape=(num_labels,))\n",
    "        encoded_repr = self.encoder(img)\n",
    "        gen_img = self.decoder([encoded_repr, label])\n",
    "        self.autoencoder = Model([img, label], gen_img)\n",
    "        valid = self.discriminator(encoded_repr)\n",
    "        self.encoder_discriminator = Model(img, valid)\n",
    "        self.discriminator.compile(optimizer=self.optimizer_discriminator,\n",
    "                                   loss='binary_crossentropy',\n",
    "                                   metrics=['accuracy'])\n",
    "        self.autoencoder.compile(optimizer=self.optimizer_reconst,\n",
    "                                 loss ='mse')\n",
    "        for layer in self.discriminator.layers:\n",
    "            layer.trainable = False\n",
    "        self.encoder_discriminator.compile(optimizer=self.optimizer_discriminator,\n",
    "                                           loss='binary_crossentropy',\n",
    "                                           metrics=['accuracy'])\n",
    "    def imagegrid(self, epochnumber):\n",
    "        fig = plt.figure(figsize=[20, 20])\n",
    "        labels = y_train\n",
    "        styles = np.random.normal(size=(10, self.encoded_dim))\n",
    "        k=0\n",
    "        for label in labels[epochnumber/num_labels:epochnumber/num_labels+10]:\n",
    "            for style in styles:\n",
    "                img = self.decoder.predict([style.reshape(-1,self.encoded_dim), label.reshape(-1,num_labels)])\n",
    "                img = img.reshape(input_shape)\n",
    "                k = k + 1\n",
    "                # 250 rows, 10 colums\n",
    "                ax = fig.add_subplot(10, 10, k)\n",
    "                ax.set_axis_off()\n",
    "                ax.imshow(img, cmap=\"gray\")\n",
    "        fig.savefig(\"images/SAAE/\" + str(epochnumber) + \".png\")\n",
    "        plt.show()\n",
    "        plt.close(fig)\n",
    "\n",
    "\n",
    "    def train(self, x_train, y_train, batch_size=32, epochs=5000, save_image_interval=10):\n",
    "        labels = y_train\n",
    "        for epoch in range(epochs):\n",
    "            #---------------Train Discriminator -------------\n",
    "            # Select a random half batch of images\n",
    "            idx = np.random.randint(0, x_train.shape[0], batch_size)\n",
    "            imgs = x_train[idx]\n",
    "            labels_batch = labels[idx, :]\n",
    "            # Generate a half batch of new images\n",
    "            latent_fake = self.encoder.predict(imgs)\n",
    "            #gen_imgs = self.decoder.predict(latent_fake)\n",
    "            latent_real = np.random.normal(size=(batch_size, self.encoded_dim))\n",
    "            valid = np.ones((batch_size, 1))\n",
    "            fake = np.zeros((batch_size, 1))\n",
    "            # Train the discriminator\n",
    "            d_loss_real = self.discriminator.train_on_batch(latent_real, valid)\n",
    "            d_loss_fake = self.discriminator.train_on_batch(latent_fake, fake)\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "            #idx = np.random.randint(0, x_train.shape[0], batch_size)\n",
    "            #imgs = x_train[idx]\n",
    "            # Generator wants the discriminator to label the generated representations as valid\n",
    "            valid_y = np.ones((batch_size, 1))\n",
    "\n",
    "            # Train the autoencode reconstruction\n",
    "            g_loss_reconstruction = self.autoencoder.train_on_batch([imgs, labels_batch], imgs)\n",
    "\n",
    "            # Train generator\n",
    "            g_logg_similarity = self.encoder_discriminator.train_on_batch(imgs, valid_y)\n",
    "            # Plot the progress\n",
    "            print (\"%d [D loss: %f, acc: %.2f%%] [G acc: %f, mse: %f]\" % (epoch, d_loss[0], 100*d_loss[1],\n",
    "                   g_logg_similarity[1], g_loss_reconstruction))\n",
    "            if(epoch % save_image_interval == 0):\n",
    "                self.imagegrid(epoch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "reasonable-disease",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SAAE' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-1-528512538a82>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mann\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mSAAE\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0mann\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtrain\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx_train\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my_train\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbatch_size\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mepochs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'SAAE' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "ann = SAAE()\n",
    "ann.train(x_train, y_train, batch_size, epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legal-crazy",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}